{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1164617,"sourceType":"datasetVersion","datasetId":659784},{"sourceId":6143338,"sourceType":"datasetVersion","datasetId":3522839},{"sourceId":6571530,"sourceType":"datasetVersion","datasetId":3796024},{"sourceId":7017419,"sourceType":"datasetVersion","datasetId":3937250},{"sourceId":7244751,"sourceType":"datasetVersion","datasetId":4196596},{"sourceId":7254401,"sourceType":"datasetVersion","datasetId":2820075},{"sourceId":7345392,"sourceType":"datasetVersion","datasetId":4265231},{"sourceId":7395000,"sourceType":"datasetVersion","datasetId":4299398},{"sourceId":7396579,"sourceType":"datasetVersion","datasetId":4300571},{"sourceId":7399589,"sourceType":"datasetVersion","datasetId":4302529},{"sourceId":7433684,"sourceType":"datasetVersion","datasetId":4326037},{"sourceId":8035869,"sourceType":"datasetVersion","datasetId":4737133},{"sourceId":8089862,"sourceType":"datasetVersion","datasetId":4775972},{"sourceId":157031407,"sourceType":"kernelVersion"},{"sourceId":159736702,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U peft --no-index --find-links /kaggle/input/llm-detect-pip-rb\n!pip install -q -U accelerate --no-index --find-links /kaggle/input/llm-detect-pip-rb\n!pip install -q -U bitsandbytes --no-index --find-links /kaggle/input/llm-detect-pip-rb\n!pip install -q -U transformers --no-index --find-links /kaggle/input/llm-detect-pip-rb\n!pip install -q /kaggle/input/sparse-dot-topn-033/sparse_dot_topn-0.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-11T10:06:38.917202Z","iopub.execute_input":"2024-04-11T10:06:38.917701Z","iopub.status.idle":"2024-04-11T10:07:44.353958Z","shell.execute_reply.started":"2024-04-11T10:06:38.917674Z","shell.execute_reply":"2024-04-11T10:07:44.352708Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.makedirs(\"./outputs\", exist_ok=True)\nos.makedirs(\"./configs\", exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:15:08.795982Z","iopub.execute_input":"2024-04-11T10:15:08.796745Z","iopub.status.idle":"2024-04-11T10:15:08.801394Z","shell.execute_reply.started":"2024-04-11T10:15:08.796710Z","shell.execute_reply":"2024-04-11T10:15:08.800357Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%%writefile run_llm_inference.py\n\nimport sys\n\nsys.path.insert(0, '/kaggle/input/omegaconf')\nsys.path.insert(0, '/kaggle/input/utils-ai-v3')\n\nimport argparse\nimport os\n\nimport pandas as pd\nimport torch\nfrom accelerate import Accelerator\nfrom omegaconf import OmegaConf\nfrom peft import PeftModel\n\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n\nfrom r_detect.ai_dataset import AiDataset\nfrom r_detect.ai_loader import AiCollator, show_batch\nfrom r_detect.ai_model import MistralForDetectAI\n\nimport re\nimport unicodedata\n\n\n# pre-process -----\nchar_to_remove = ['{', '£', '\\x97', '¹', 'å', '\\\\', '\\x85', '<', '\\x99', \\\n                  'é', ']', '+', 'Ö', '\\xa0', '>', '|', '\\x80', '~', '©', \\\n                  '/', '\\x93', '$', 'Ó', '²', '^', ';', '`', 'á', '*', '(', \\\n                  '¶', '®', '[', '\\x94', '\\x91', '#', '-', 'ó', ')', '}', '=']\n\n\n\ndef preprocess_text(text, strategy='light'):\n    assert strategy in [\"none\", \"light\", \"heavy\"], \"pre-processing strategy must one of: none, light, heavy\"\n    \n    if strategy == \"none\":\n        text = text\n        \n    elif strategy == \"light\":\n        text = text.encode(\"ascii\", \"ignore\").decode('ascii')        \n        text = text.strip()\n        text = text.strip(\"\\\"\")\n\n        for c in char_to_remove:\n            text = text.replace(c, \"\")\n\n        if text[-1]!=\".\":\n            text = text.split(\".\")\n            text = \".\".join(text[:-1])\n            text += \".\"\n    else:\n        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n        text = text.lower()\n        text = re.sub(r'[^a-z0-9\\s.,;?!:()\\'\\\"%-]', '', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n\ndef run_inference(accelerator, model, infer_dl, example_ids):\n    model.eval()\n    all_predictions = []\n\n    progress_bar = tqdm(range(len(infer_dl)), disable=not accelerator.is_local_main_process)\n\n    for step, batch in enumerate(infer_dl):\n        with torch.no_grad():\n            outputs = model(**batch)\n\n        logits = outputs.logits.reshape(-1)\n        predictions = torch.sigmoid(logits)\n        predictions = accelerator.gather_for_metrics(predictions)\n        predictions = predictions.cpu().numpy().tolist()\n\n        all_predictions.extend(predictions)\n\n        progress_bar.update(1)\n    progress_bar.close()\n\n    result_df = pd.DataFrame()\n    result_df[\"id\"] = example_ids\n    result_df[\"generated\"] = all_predictions\n\n    return result_df\n\ndef main(cfg, save_dir, model_id):\n    \n    # create accelerator\n    accelerator = Accelerator()\n    \n    # read test data\n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        test_df = pd.read_csv(\"/kaggle/input/textdata/test_essays.csv\", sep=',')\n    else:\n        test_df = pd.read_csv(\"/kaggle/input/textdata/test.csv\", sep=',')\n        \n    accelerator.print(\"~~\"*40)\n    accelerator.print(f\"PRE-PROCESSING: {cfg.preprocess_strategy.upper()}\")\n    accelerator.print(\"~~\"*40)\n\n    test_df['text'] = test_df['text'].apply(lambda x: preprocess_text(x, cfg.preprocess_strategy))\n    accelerator.print(f'Test csv shape: {test_df.shape}')\n    \n    with accelerator.main_process_first():\n        dataset_creator = AiDataset(cfg)\n        infer_ds = dataset_creator.get_dataset(test_df)\n    \n    tokenizer = dataset_creator.tokenizer\n    # tokenizer.pad_token = tokenizer.eos_token\n    \n    infer_ds = infer_ds.sort(\"input_length\")\n    infer_ds.set_format(\n        type=None,\n        columns=[\n            'id',\n            'input_ids',\n            'attention_mask',\n        ]\n    )\n    \n    infer_ids = infer_ds[\"id\"]  # .tolist()\n    \n    #--\n    data_collator = AiCollator(\n        tokenizer=tokenizer,\n        pad_to_multiple_of=64\n    )\n\n    infer_dl = DataLoader(\n        infer_ds,\n        batch_size=cfg.predict_params.per_device_eval_batch_size,\n        shuffle=False,\n        collate_fn=data_collator,\n    )\n\n    accelerator.print(\"data preparation done...\")\n    accelerator.print(\"~~\"*40)\n    accelerator.wait_for_everyone()\n    \n    \n    #----------\n    for b in infer_dl:\n        break\n    show_batch(b, tokenizer, task='infer', print_fn=accelerator.print)\n    accelerator.print(\"~~\"*40)\n    #----------\n\n\n    ## Load Model\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_compute_dtype=torch.float16\n    )\n    \n    base_model = MistralForDetectAI.from_pretrained(\n        cfg.model.backbone_path,\n        num_labels=cfg.model.num_labels,\n        quantization_config=bnb_config,\n        low_cpu_mem_usage=True\n    )\n    \n    base_model.config.pretraining_tp = 1\n    # base_model.config.pad_token_id = tokenizer.pad_token_id\n    model = PeftModel.from_pretrained(base_model, cfg.model.lora_path)\n    accelerator.print(\"### Loaded Model Weights ###\")\n    \n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    # run inference ---\n    sub_df = run_inference(accelerator, model, infer_dl, infer_ids)\n    accelerator.wait_for_everyone()\n    \n    if accelerator.is_main_process:\n        save_path = os.path.join(save_dir, f\"{model_id}.parquet\")\n        sub_df.to_parquet(save_path)\n        accelerator.print(\"done!\")\n        accelerator.print(\"~~\"*40)\n    \nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument('--config_path', type=str, required=True)\n    ap.add_argument('--save_dir', type=str, required=True)\n    ap.add_argument('--model_id', type=str, required=True)\n\n    args = ap.parse_args()\n    cfg = OmegaConf.load(args.config_path)\n\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    # execution\n    main(\n        cfg,\n        save_dir=args.save_dir,\n        model_id=args.model_id,\n    )\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:15:11.253362Z","iopub.execute_input":"2024-04-11T10:15:11.253973Z","iopub.status.idle":"2024-04-11T10:15:11.263268Z","shell.execute_reply.started":"2024-04-11T10:15:11.253941Z","shell.execute_reply":"2024-04-11T10:15:11.262328Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Overwriting run_llm_inference.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile ./configs/conf_mistral_mix16.yaml\n\npreprocess_strategy: light\n\nmodel:\n    backbone_path: \"/kaggle/input/mistral-7b-v0-1/Mistral-7B-v0.1\"\n    lora_path: /kaggle/input/detect-ai-r-detect-v16-r8\n    max_length: 1296\n    num_labels: 1\n    tokenizer:\n        padding_side: left\n        truncation_side: left\n        use_fast: true\n\npredict_params:\n    per_device_eval_batch_size: 1\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:15:17.252987Z","iopub.execute_input":"2024-04-11T10:15:17.253632Z","iopub.status.idle":"2024-04-11T10:15:17.260418Z","shell.execute_reply.started":"2024-04-11T10:15:17.253598Z","shell.execute_reply":"2024-04-11T10:15:17.259539Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Writing ./configs/conf_mistral_mix16.yaml\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile ./configs/conf_mistral_v26.yaml\n\npreprocess_strategy: none\n\nmodel:\n    backbone_path: \"/kaggle/input/mistral-7b-v0-1/Mistral-7B-v0.1\"\n    lora_path: /kaggle/input/detect-ai-r-detect-v26\n    max_length: 1296\n    num_labels: 1\n    tokenizer:\n        padding_side: left\n        truncation_side: left\n        use_fast: true\n\npredict_params:\n    per_device_eval_batch_size: 1","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:15:20.347215Z","iopub.execute_input":"2024-04-11T10:15:20.347876Z","iopub.status.idle":"2024-04-11T10:15:20.353762Z","shell.execute_reply.started":"2024-04-11T10:15:20.347847Z","shell.execute_reply":"2024-04-11T10:15:20.352778Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Writing ./configs/conf_mistral_v26.yaml\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 run_llm_inference.py \\\n--config_path \"./configs/conf_mistral_mix16.yaml\" \\\n--save_dir \"./outputs\" \\\n--model_id \"m0\"","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:15:23.238158Z","iopub.execute_input":"2024-04-11T10:15:23.239196Z","iopub.status.idle":"2024-04-11T10:22:33.527861Z","shell.execute_reply.started":"2024-04-11T10:15:23.239159Z","shell.execute_reply":"2024-04-11T10:22:33.526625Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"2024-04-11 10:15:39.982959: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 10:15:39.982970: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 10:15:39.983026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 10:15:39.983081: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 10:15:40.071965: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-04-11 10:15:40.071974: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nPRE-PROCESSING: LIGHT\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nTest csv shape: (150, 3)\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  7.07ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.96ba/s]\ndata preparation done...\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  7.24ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.16ba/s]\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nbatch size: 1\nshape of input_ids: torch.Size([1, 128])\nShowing 1 from a infer batch...\n\n\n\nExample 1\nInput:\n\n<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> Would you ever make something that would put you in any kind of harm ?. Making Drivlaby a necessity.\n\nDriverless cars must be great talk about how saving people in to today's society in way of communicating vehicles, which makes it good to actually give you our roads now.\n###\nIs the essay generated by AI?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards: 100%|█████████████████| 2/2 [05:52<00:00, 176.07s/it]\nSome weights of MistralForDetectAI were not initialized from the model checkpoint at /kaggle/input/mistral-7b-v0-1/Mistral-7B-v0.1 and are newly initialized: ['classification_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nLoading checkpoint shards: 100%|█████████████████| 2/2 [05:53<00:00, 176.59s/it]\nSome weights of MistralForDetectAI were not initialized from the model checkpoint at /kaggle/input/mistral-7b-v0-1/Mistral-7B-v0.1 and are newly initialized: ['classification_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n### Loaded Model Weights ###\n100%|███████████████████████████████████████████| 75/75 [00:41<00:00,  1.81it/s]\ndone!\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nCPU times: user 4.86 s, sys: 1.11 s, total: 5.97 s\nWall time: 7min 10s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 run_llm_inference.py \\\n--config_path \"./configs/conf_mistral_v26.yaml\" \\\n--save_dir \"./outputs\" \\\n--model_id \"m1\"\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:24:12.101154Z","iopub.execute_input":"2024-04-11T10:24:12.101882Z","iopub.status.idle":"2024-04-11T10:25:38.551419Z","shell.execute_reply.started":"2024-04-11T10:24:12.101841Z","shell.execute_reply":"2024-04-11T10:25:38.550229Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"2024-04-11 10:24:23.411775: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 10:24:23.411833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 10:24:23.413479: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-04-11 10:24:23.427159: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 10:24:23.427199: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 10:24:23.428567: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nPRE-PROCESSING: NONE\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nTest csv shape: (150, 3)\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.23ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.94ba/s]\ndata preparation done...\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.99ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.02ba/s]\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nbatch size: 1\nshape of input_ids: torch.Size([1, 128])\nShowing 1 from a infer batch...\n\n\n\nExample 1\nInput:\n\n<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> Would you ever make something that would put you in any kind of harm ?. Making Drivlaby a necessity.\n\nDriverless cars must be great talk about how saving people in to today's society in way of communicating vehicles, which makes it good to actually give you our roads now.\n###\nIs the essay generated by AI?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:17<00:00,  9.00s/it]\nSome weights of MistralForDetectAI were not initialized from the model checkpoint at /kaggle/input/mistral-7b-v0-1/Mistral-7B-v0.1 and are newly initialized: ['classification_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:17<00:00,  8.97s/it]\nSome weights of MistralForDetectAI were not initialized from the model checkpoint at /kaggle/input/mistral-7b-v0-1/Mistral-7B-v0.1 and are newly initialized: ['classification_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n### Loaded Model Weights ###\n100%|███████████████████████████████████████████| 75/75 [00:41<00:00,  1.82it/s]\ndone!\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nCPU times: user 1.29 s, sys: 249 ms, total: 1.54 s\nWall time: 1min 26s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile run_deberta_inference_ub.py\n\nimport os\nimport torch\nimport argparse\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom scipy.special import expit\nfrom transformers import (\n    DataCollatorWithPadding, TrainingArguments,\n    AutoTokenizer, AutoModelForSequenceClassification, Trainer\n)\n\nimport gc\nimport torch\n\nfrom accelerate import Accelerator\naccelerator = Accelerator()\n\ndef preprocess_function(examples, max_length, tokenizer):\n    tokenized_samples = tokenizer(examples[\"text\"], truncation=True, max_length=max_length)\n    return tokenized_samples\n\ndef main(args):\n    # read test data\n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        test_df = pd.read_csv(\"/kaggle/input/textdata/test_essays.csv\", sep=',')\n    else:\n        test_df = pd.read_csv(\"/kaggle/input/textdata/test.csv\", sep=',')\n        \n    accelerator.print(f'Test csv shape: {test_df.shape}')\n    test_ds = Dataset.from_pandas(test_df)\n\n    ## Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(args.base_model_path)\n\n    test_tokenized_ds = test_ds.map(preprocess_function, batched=True, \n                                    fn_kwargs={\"max_length\": args.max_length, \"tokenizer\": tokenizer},\n                                    remove_columns=test_ds.column_names)\n    \n    for idx in range(2):\n        accelerator.print(f\"\\n--- Sample {idx} ---\\n\")\n        accelerator.print(tokenizer.decode(test_tokenized_ds[idx][\"input_ids\"]))\n\n    ## Load Model\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        args.base_model_path,\n        num_labels=1\n    )\n    model = accelerator.prepare(model)\n    accelerator.print(\"### Loaded Model Weights ###\")\n\n    ## Trainer Setup\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n    training_args = TrainingArguments(output_dir=\"tmp\", \n                                  per_device_eval_batch_size=1,\n                                  remove_unused_columns=False)\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    ## predictor\n    pred_output = trainer.predict(test_tokenized_ds)\n    logits = pred_output.predictions.astype(float)\n#     probs = expit(logits)[:, 0]\n    probs = -1 * logits[:, 0]\n\n    sub = pd.DataFrame({\n        \"id\": test_df['id'].values,\n        \"generated\": probs\n    })\n    save_path = os.path.join(args.save_dir, f\"{args.model_id}.parquet\")\n    sub.to_parquet(save_path)\n\nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument('--base_model_path', type=str, required=True)\n    ap.add_argument('--max_length', type=int, required=True)\n    ap.add_argument('--save_dir', type=str, default=\"./outputs\")\n    ap.add_argument('--model_id', type=str, required=True)\n    args = ap.parse_args()\n    \n    main(args)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:26:03.638331Z","iopub.execute_input":"2024-04-11T10:26:03.639047Z","iopub.status.idle":"2024-04-11T10:26:03.646882Z","shell.execute_reply.started":"2024-04-11T10:26:03.639007Z","shell.execute_reply":"2024-04-11T10:26:03.646002Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Writing run_deberta_inference_ub.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 run_deberta_inference_ub.py \\\n--base_model_path \"/kaggle/input/deberta-v3-large-v18-margin/checkpoint-6250\" \\\n--max_length 1024 \\\n--save_dir \"./outputs\" \\\n--model_id \"m2\"\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:26:18.837542Z","iopub.execute_input":"2024-04-11T10:26:18.838249Z","iopub.status.idle":"2024-04-11T10:27:40.172791Z","shell.execute_reply.started":"2024-04-11T10:26:18.838216Z","shell.execute_reply":"2024-04-11T10:27:40.171603Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"2024-04-11 10:26:29.026637: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 10:26:29.026701: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 10:26:29.028174: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-04-11 10:26:29.033036: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 10:26:29.033080: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 10:26:29.034415: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTest csv shape: (150, 3)\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.00ba/s]\n\n--- Sample 0 ---\n\n[CLS] #Technology Has Changed Over Past Decade Essay (EasyBib) ##Introduction Using technology to read the emotional expressions of students in a classroom is valuable because it allows instructors to provide needed support and direction to struggling learners without having to resort to physical intervention methods such as verbal commands or personal touch-points which often take away from instructional focus. This proposed application requires little change in current teaching practices; however, does require some investment of resources into the training of staff members to operate effectively in the classroom setting. Additionally, there are many benefits associated with implementing these technologies within educational institutions including cost savings through decreased workloads and increased efficiency. Finally, I would suggest that research studies indicate that positive outcomes may occur even though only one third of learning experiences involve direct communication between peers.\\ Research Studies and Evidence There is ample evidence supporting the effectiveness of “computer based programs” in improving reading comprehension skills among all types of learners regardless of language proficiency level. Research indicates that interactive software applications developed specifically for children improve vocabulary acquisition scores compared to those who receive traditional paper text books and/or textbook supplements (Fletcher et al., 2001). However, more importantly than academic gains realized via these computer assisted applications, there also appear to be other factors at play here that enhance overall student satisfaction levels. In fact, according to Fletcher et al. (2001), high school students participating in online courses had significantly higher grades and reported less negative attitudes toward school in general. These authors concluded that while the initial financial costs associated with installing and operating the program were daunting, long term economic rewards far outweighing the upfront expenditures made possible by a well designed curriculum development process. As we move forward towards future technological advancements regarding artificial intelligence systems capable of synthesizing spoken words directly into written documents, we must realize that no matter what type of system is implemented, its efficacy relies largely upon thorough content preparation combined with careful testing before implementation becomes commonplace across various disciplines throughout our society. As an educator specializing in speech pathology, my goal is to make sure every child receives quality education services during his/her critical years of development so that he/she will become productive and contributing member of our society later in life. If adopted successfully, this approach would allow me to achieve the best outcome available, thereby enabling the maximum number of individuals to reap the potential benefit offered by improved literacy capabilities – resulting in an enhanced capacity for understanding information presented in print form as well as oral presentations. Moreover, ensuring that every student’s needs are addressed first and foremost prior to determining appropriate interventions helps create opportunities for creative problem solving strategies instead of simply placing blame and assigning labels. In addition, providing immediate feedback and assistance along the way serves to reduce frustration and anxiety experienced by each student as their success stories grow increasingly visible over time. Ultimately, this method ensures individualized care and attention is provided in order to maximize progress and achievement goals set forth at the beginning.\\ Technological Advancement Over the last ten years alone, significant advances have taken place in regards to computational power coupled with improvements in microprocessor speed, memory size and processing speeds. Combined together, these elements enable scientists and engineers to develop applications and processes that perform tasks previously thought impossible to complete within reasonable periods of time. We currently live in an age where computers and networks function with amazing precision allowing users around the globe to access vast amounts of information instantly – a feat unheard of twenty five years ago! Likewise, intelligent interfaces incorporated within handheld devices offer unprecedented functionality never before seen outside of science fiction movies. Unfortunately, much of the world remains disconnected from modern conveniences thanks to insufficient funding or lack of adequate infrastructure that supports global telecommunications standards. However, I believe these challenges will soon be overcome once sufficient interest is generated amongst governments, businesses and consumers alike leading us closer to[SEP]\n\n--- Sample 1 ---\n\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.22ba/s]\n[CLS] The use of the Facial Action Coding System can be valuable to read the emotional expressions of students in a classroom because we can know how that person is feeling at the time. Would you like to know if a person is feeling sad,happy,bored, or even angry? Of course,not only do we know how they feel but we can also take a step futher on helping them. If a person is depressed we can talk to someone close from them and explain what they are going through, or advice them for that person to take therapy. The Facial Action Coding System is also a great start to poeple who are new to school or even to sports. If a person feels sad because they arent doing as good as their other teamates, The coach should use the software because it will let him/her know why that person isnt really trying in the sport. Another reason why im for it is because teachers really dont know when you are bored or confused. Especially if you are one of those shy prsons that dont like raising their hand. By using this software teachers can go up to you and explain you with what you are having trouble. If a student is bored with what the what teacher is teaching, she should probably try something new to get the students attention. This technology can also show us when someone has a fake smile for example if you think that you have a friend that really isnt your friend and you talk to her she might be giving you a fake smile because she probably doesnt like you. The use of this software can determine if you are having problems in your house,the expressions that will show would be sadness and probably anger. In todays world you may be going through a lot but people may not know and they would treat you like garbage. This would then cause you depression and probably then commiting suicide. Having this technology \"Facial Action Coding System\" in hands can help you stop somone from danger. having this technology can ressolve a lot of problems with putting yourself into it. Every perosn has a falling action and a resolution to their story. With just a facial expression it can change realtionships and friendships.\"Making people smile\" is what we need with Facial Action Coding System![SEP]\n### Loaded Model Weights ###\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 11.47it/s]\nCPU times: user 932 ms, sys: 232 ms, total: 1.16 s\nWall time: 1min 21s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile run_deberta_mlm.py\nimport sys\nsys.path.insert(0, '/kaggle/input/omegaconf')\n\nimport argparse\nimport math\nimport os\nimport time\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom itertools import chain\nfrom typing import Optional\n\nimport bitsandbytes as bnb\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom datasets import Dataset\nfrom omegaconf import OmegaConf\nfrom tokenizers import (Tokenizer, models, normalizers, pre_tokenizers,\n                        processors, trainers)\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom tqdm.auto import tqdm\nfrom transformers import (DataCollatorForLanguageModeling, DebertaV2Config,\n                          DebertaV2ForMaskedLM, PreTrainedTokenizerBase,\n                          PreTrainedTokenizerFast, default_data_collator,\n                          get_cosine_schedule_with_warmup)\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import \\\n    DebertaV2OnlyMLMHead\nfrom transformers.trainer_pt_utils import get_parameter_names\n\n# utils ---------------------------------------------------------------------------------#\n\n\ndef as_minutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm%ds' % (m, s)\n\n\ndef get_lr(optimizer):\n    return optimizer.param_groups[0]['lr']*1e6\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\n       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n# tokenizer class -----------------------------------------------------------------------#\n\n\nclass BPETokenizer:\n    ST = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\", \"[MASK]\"]\n\n    def __init__(self, vocab_size):\n        self.vocab_size = vocab_size\n        self.tok = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n        self.tok.normalizer = normalizers.Sequence([normalizers.NFC()])\n        self.tok.pre_tokenizer = pre_tokenizers.ByteLevel()\n        self.tok.post_processor = processors.TemplateProcessing(\n            single=\"[CLS] $0 [SEP]\",\n            pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n            special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n        )\n\n    @classmethod\n    def chunk_dataset(cls, dataset, chunk_size=1_000):\n        for i in range(0, len(dataset), chunk_size):\n            yield dataset[i: i + chunk_size][\"text\"]\n\n    def train(self, data):\n        trainer = trainers.BpeTrainer(vocab_size=self.vocab_size, special_tokens=self.ST)\n        dataset = Dataset.from_pandas(data[[\"text\"]])\n        self.tok.train_from_iterator(self.chunk_dataset(dataset), trainer=trainer)\n        return self\n\n    def tokenize(self, data):\n        tokenized_texts = []\n        for text in tqdm(data['text'].tolist()):\n            tokenized_texts.append(self.tok.encode(text))\n        return tokenized_texts\n\n    def get_fast_tokenizer(self, max_length):\n        return PreTrainedTokenizerFast(\n            tokenizer_object=self.tok,\n            unk_token=\"[UNK]\",\n            pad_token=\"[PAD]\",\n            cls_token=\"[CLS]\",\n            sep_token=\"[SEP]\",\n            mask_token=\"[MASK]\",\n            model_max_length=max_length\n        )\n\n\ndef tokenizer_test(tokenizer):\n    print(\"==\"*40)\n    print(f\"tokenizer len: {len(tokenizer)}\")\n    test_string = \"This is a test \\n\\n!!\"\n    print(f\"tokenizer test: {tokenizer.tokenize(test_string)}\")\n    print(\"==\"*40)\n\n# collator class ------------------------------------------------------------------------#\n\n\n@dataclass\nclass CustomDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n    tokenizer: PreTrainedTokenizerBase\n    mlm: bool = True\n    mlm_probability: float = 0.15\n    pad_to_multiple_of: Optional[int] = None\n    tf_experimental_compile: bool = False\n    return_tensors: str = \"pt\"\n\n    def torch_mask_tokens(self, inputs, special_tokens_mask):\n        \"\"\"\n        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n        \"\"\"\n\n        labels = inputs.clone()\n\n        # geometric distribution for spans\n        geo_p, lower, upper = 0.15, 1, 6\n        len_distrib = [geo_p * (1-geo_p)**(i - lower) for i in range(lower, upper + 1)]\n        len_distrib = [x / (sum(len_distrib)) for x in len_distrib]\n        lens = list(range(lower, upper + 1))\n\n        masked_indices = []\n\n        for ex_labels in labels:\n            mask_num = math.ceil(len(ex_labels) * self.mlm_probability)\n            ex_mask = set()\n            while len(ex_mask) < mask_num:\n                span_len = np.random.choice(lens, p=len_distrib)\n                anchor = np.random.choice(len(ex_labels))\n                if anchor in ex_mask:\n                    continue\n                else:\n                    left1, right1 = anchor, min(anchor + span_len, len(ex_labels))\n                    for i in range(left1, right1):\n                        if len(ex_mask) >= mask_num:\n                            break\n                        ex_mask.add(i)\n            ex_mask_bool = [i in ex_mask for i in range(len(ex_labels))]\n            masked_indices.append(ex_mask_bool)\n        masked_indices = torch.tensor(masked_indices).bool()\n\n        if special_tokens_mask is None:\n            special_tokens_mask = [\n                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n            ]\n            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n        else:\n            special_tokens_mask = special_tokens_mask.bool()\n\n        masked_indices = torch.logical_and(masked_indices, ~special_tokens_mask)\n        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n\n        # 98% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.98)).bool() & masked_indices\n        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n\n        # 1% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n        # pdb.set_trace()\n\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels\n\n\ndef show_batch(batch, tokenizer, num_examples=8, print_fn=print):\n    print_fn('=='*40)\n    num_examples = min(num_examples, len(batch['input_ids']))\n\n    for i in range(num_examples):\n        input_ids = batch['input_ids'][i]\n        input_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n        print_fn(f\"input text:\\n {input_text}\")\n        print_fn('=='*40)\n\n# dataset class -------------------------------------------------------------------------#\n\n\ndef get_mlm_dataset(cfg, notes_df, tokenizer):\n    notes_df = notes_df[['text']].copy()\n    notes_df = notes_df.reset_index(drop=True)\n\n    task_dataset = Dataset.from_pandas(notes_df)\n\n    def tokenize_function(examples):\n        result = tokenizer(examples['text'])\n        return result\n\n    tokenized_datasets = task_dataset.map(\n        tokenize_function, batched=True, remove_columns=task_dataset.column_names\n    )\n\n    chunk_size = cfg.max_length\n\n    def group_texts(examples):\n        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        total_length = (total_length // chunk_size) * chunk_size\n\n        result = {\n            k: [t[i: i + chunk_size] for i in range(0, total_length, chunk_size)]\n            for k, t in concatenated_examples.items()\n        }\n\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n\n    test_pct = cfg.test_pct\n\n    max_train_examples = cfg.max_train_examples\n    max_test_examples = int(max_train_examples * test_pct)\n\n    test_size = int(len(lm_datasets) * test_pct)\n    train_size = len(lm_datasets) - test_size\n\n    test_size = min(test_size, max_test_examples)\n    train_size = min(train_size, max_train_examples)\n\n    downsampled_dataset = lm_datasets.train_test_split(\n        train_size=train_size, test_size=test_size, seed=cfg.seed\n    )\n    data_collator = CustomDataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm_probability=cfg.mask_probability\n    )\n\n    def insert_random_mask(batch):\n        features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n        masked_inputs = data_collator(features)\n        return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n\n    downsampled_dataset[\"test\"] = downsampled_dataset[\"test\"].map(\n        insert_random_mask,\n        batched=True,\n        remove_columns=downsampled_dataset[\"test\"].column_names,\n    )\n\n    try:\n        downsampled_dataset[\"test\"] = downsampled_dataset[\"test\"].rename_columns(\n            {\n                \"masked_input_ids\": \"input_ids\",\n                \"masked_attention_mask\": \"attention_mask\",\n                \"masked_labels\": \"labels\",\n                \"masked_token_type_ids\": \"token_type_ids\",\n            }\n        )\n    except Exception as e:\n        downsampled_dataset[\"test\"] = downsampled_dataset[\"test\"].rename_columns(\n            {\n                \"masked_input_ids\": \"input_ids\",\n                \"masked_attention_mask\": \"attention_mask\",\n                \"masked_labels\": \"labels\",\n            }\n        )\n\n    return downsampled_dataset\n\n# main ----------------------------------------------------------------------------------#\n\n\ndef main(cfg):\n    accelerator = Accelerator(\n        gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n        mixed_precision='fp16',\n    )\n\n    def print_line():\n        prefix, unit, suffix = \"#\", \"~~\", \"#\"\n        accelerator.print(prefix + unit*50 + suffix)\n\n    # set seed ----\n    print_line()\n    accelerator.print(f\"setting seed: {cfg.seed}\")\n    set_seed(cfg.seed)\n\n    if accelerator.is_main_process:\n        os.makedirs(cfg.model_dir, exist_ok=True)\n    print_line()\n\n    # data ------------------------------------------------------------------------------#\n    train_df = pd.read_csv(cfg.train_data_path).rename(columns={'full_text': 'text', 'essay_id_comp': 'id'})\n    train_df = train_df[['id', 'text']].copy()\n    \n    # test_df = pd.read_csv(cfg.test_data_path)\n    \n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        test_df = pd.read_csv(\"/kaggle/input/textdata/test_essays.csv\", sep=',')\n    else:\n        test_df = pd.read_csv(\"/kaggle/input/textdata/test.csv\", sep=',')\n    \n    \n    test_df = test_df[['id', 'text']].copy()\n\n    train_df = train_df.sample(frac=cfg.train_frac_for_mlm).reset_index(drop=True)\n\n    if cfg.debug:\n        n_debug = min(1000, len(train_df))\n        train_df = train_df.sample(n_debug, random_state=cfg.seed).reset_index(drop=True)\n\n    notes_df = pd.concat([train_df, test_df, test_df], axis=0).reset_index(drop=True)  # oversample test 2x\n    accelerator.print(f\"shape of input text data: {notes_df.shape}\")\n    print_line()\n\n    # tokenizer -------------------------------------------------------------------------#\n    tok_train_df = train_df.sample(\n        frac=cfg.train_frac_for_tokenizer, random_state=cfg.seed\n    ).reset_index(drop=True)\n    accelerator.print(f\"Train tokenizer with {len(tok_train_df)} train df samples\")\n\n    bpe_tok = BPETokenizer(cfg.vocab_size).train(\n        pd.concat((tok_train_df, test_df)).reset_index(drop=True)\n    )\n    tokenizer = bpe_tok.get_fast_tokenizer(cfg.max_length)\n\n    # dataset ---------------------------------------------------------------------------#\n    with accelerator.main_process_first():\n        mlm_dataset = get_mlm_dataset(cfg, notes_df, tokenizer)\n\n    # model------------------------------------------------------------------------------#\n    base_config = DebertaV2Config(\n        attention_probs_dropout_prob=0.0,\n        hidden_act=\"gelu\",\n        hidden_dropout_prob=0.0,\n        hidden_size=768,\n        initializer_range=0.02,\n        intermediate_size=3072,\n        max_position_embeddings=512,\n        relative_attention=True,\n        position_buckets=256,\n        norm_rel_ebd=\"layer_norm\",\n        share_att_key=True,\n        pos_att_type=\"p2c|c2p\",\n        layer_norm_eps=1e-7,\n        max_relative_positions=-1,\n        position_biased_input=False,\n        num_attention_heads=12,\n        num_hidden_layers=6,\n        type_vocab_size=0,\n        vocab_size=tokenizer.vocab_size,\n    )\n\n    model = DebertaV2ForMaskedLM(base_config)\n    model.deberta.resize_token_embeddings(len(tokenizer))\n    model.cls = DebertaV2OnlyMLMHead(base_config)\n\n    # optimizer -------------------------------------------------------------------------#\n    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n            \"weight_decay\": cfg.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n            \"weight_decay\": 0.0,\n        },\n    ]\n\n    accelerator.print(\"using bnb optimizer....\")\n\n    optimizer = bnb.optim.Adam8bit(\n        optimizer_grouped_parameters, lr=cfg.lr,\n    )\n\n    # collator --------------------------------------------------------------------------#\n\n    eval_dataset = deepcopy(mlm_dataset['test'])\n\n    data_collator = CustomDataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm_probability=cfg.mask_probability\n    )\n\n    batch_size = cfg.per_device_batch_size\n\n    train_dataloader = DataLoader(\n        mlm_dataset[\"train\"],\n        shuffle=True,\n        batch_size=batch_size,\n        collate_fn=data_collator,\n    )\n\n    eval_dataloader = DataLoader(\n        mlm_dataset[\"test\"],\n        batch_size=batch_size,\n        collate_fn=default_data_collator,\n    )\n\n    # show training batch ---\n    for batch in train_dataloader:\n        break\n    show_batch(batch, tokenizer, num_examples=4, print_fn=accelerator.print)\n\n    accelerator.print(f\"Train dataset size: {len(mlm_dataset['train'])}\")\n    accelerator.print(f\"Test dataset size: {len(mlm_dataset['test'])}\")\n    print_line()\n\n    # prepare ---------------------------------------------------------------------------#\n    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader\n    )\n    print_line()\n\n    # scheduler -------------------------------------------------------------------------#\n    print_line()\n    num_epochs = cfg.num_train_epochs\n    grad_accumulation_steps = cfg.gradient_accumulation_steps\n    warmup_pct = cfg.warmup_pct\n\n    num_update_steps_per_epoch = len(train_dataloader)//grad_accumulation_steps\n    num_training_steps = num_epochs * num_update_steps_per_epoch\n    num_warmup_steps = int(warmup_pct*num_training_steps)\n\n    accelerator.print(f\"# training updates per epoch: {num_update_steps_per_epoch}\")\n    accelerator.print(f\"# training steps: {num_training_steps}\")\n    accelerator.print(f\"# warmup steps: {num_warmup_steps}\")\n\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer=optimizer,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps\n    )\n    accelerator.wait_for_everyone()\n\n    # training --------------------------------------------------------------------------#\n    start_time = time.time()\n    current_iteration = 0\n\n    for epoch in range(num_epochs):\n        if epoch != 0:\n            progress_bar.close()\n\n        progress_bar = tqdm(range(num_update_steps_per_epoch), disable=not accelerator.is_local_main_process)\n        loss_meter = AverageMeter()\n        model.train()\n\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                accelerator.backward(loss)\n                if accelerator.sync_gradients:\n                    accelerator.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)  # added gradient clip\n                    optimizer.step()\n                    scheduler.step()\n                    optimizer.zero_grad()\n\n                loss_meter.update(loss.item())\n            # --\n            if accelerator.sync_gradients:\n                progress_bar.set_description(\n                    f\"STEP: {current_iteration+1:5}/{num_training_steps:5}. \"\n                    f\"LR: {get_lr(optimizer):.4f}. \"\n                    f\"Loss: {loss_meter.avg:.4f}. \"\n                )\n\n                progress_bar.update(1)\n                current_iteration += 1\n\n            # Evaluation ----\n            if (accelerator.sync_gradients) & (current_iteration % cfg.eval_frequency == 0):\n                model.eval()\n                losses = []\n\n                n_correct = 0\n                n_total = 0\n\n                for _, batch in enumerate(eval_dataloader):\n                    with torch.no_grad():\n                        outputs = model(**batch)\n\n                        tok_preds = torch.max(outputs['logits'], dim=-1)[1]\n                        curr = torch.masked_select(tok_preds == batch['labels'], batch['labels'] > -100).sum()\n                        tot = torch.masked_select(tok_preds == batch['labels'], batch['labels'] > -100).size(0)\n                        n_correct += curr\n                        n_total += tot\n\n                    loss = outputs.loss\n                    losses.append(accelerator.gather(loss.repeat(batch_size)))\n\n                losses = torch.cat(losses)\n                losses = losses[: len(eval_dataset)]\n\n                try:\n                    perplexity = math.exp(torch.mean(losses))\n                except OverflowError:\n                    perplexity = float(\"inf\")\n\n                accuracy = round((n_correct*100/n_total).item(), 2)\n                et = as_minutes(time.time()-start_time)\n                accelerator.print(\n                    f\">>> Epoch {epoch+1} | Total Step {current_iteration} | Time: {et}\"\n                )\n                accelerator.print(f\">>> Epoch {epoch+1}: Perplexity: {round(perplexity, 2)}\")\n                accelerator.print(f\">>> Epoch {epoch+1}: Accuracy: {accuracy}\")\n\n                # Save and upload ---\n                accelerator.wait_for_everyone()\n                unwrapped_model = accelerator.unwrap_model(model)\n                unwrapped_model.save_pretrained(cfg.model_dir, save_function=accelerator.save)\n                if accelerator.is_main_process:\n                    tokenizer.save_pretrained(cfg.model_dir)\n                torch.cuda.empty_cache()\n                model.train()\n                print_line()\n\n    # --- save model at the end\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(cfg.model_dir, save_function=accelerator.save)\n    if accelerator.is_main_process:\n        tokenizer.save_pretrained(cfg.model_dir)\n    torch.cuda.empty_cache()\n    model.eval()\n\n\nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument('--config_path', type=str, required=True)\n\n    args = ap.parse_args()\n    cfg = OmegaConf.load(args.config_path)\n    \n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        print(\"Setting DEBUG to False\")\n        cfg.debug = False\n\n    # execution\n    main(cfg)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:30:51.923025Z","iopub.execute_input":"2024-04-11T10:30:51.923439Z","iopub.status.idle":"2024-04-11T10:30:51.943805Z","shell.execute_reply.started":"2024-04-11T10:30:51.923405Z","shell.execute_reply":"2024-04-11T10:30:51.942826Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Writing run_deberta_mlm.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile run_train_infer_deberta.py\n\nimport sys\nsys.path.insert(0, '/kaggle/input/omegaconf')\n\nimport argparse\nimport os\nimport random\nfrom copy import deepcopy\n\nimport pandas as pd\nimport torch\nfrom accelerate import Accelerator\nfrom datasets import Dataset\nfrom omegaconf import OmegaConf\nfrom scipy.special import expit\nfrom sklearn.metrics import roc_auc_score\nfrom transformers import (AutoTokenizer, DataCollatorWithPadding,\n                          DebertaV2ForSequenceClassification, Trainer,\n                          TrainingArguments)\n\n\nclass AiDataset:\n    \"\"\"\n    Dataset class for LLM Detect AI Generated Text competition\n    \"\"\"\n\n    def __init__(self, tokenizer, max_length=1296):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def tokenize_function(self, examples):\n        tz = self.tokenizer(\n            examples[\"text\"],\n            padding=False,\n            truncation=True,\n            max_length=self.max_length,\n            add_special_tokens=True,\n            return_token_type_ids=False,\n        )\n\n        return tz\n\n    def compute_input_length(self, examples):\n        return {\"input_length\": [len(x) for x in examples[\"input_ids\"]]}\n\n    def get_dataset(self, df):\n        df = deepcopy(df)\n        task_dataset = Dataset.from_pandas(df)\n\n        task_dataset = task_dataset.map(self.tokenize_function, batched=True)\n        task_dataset = task_dataset.map(self.compute_input_length, batched=True)\n        return task_dataset\n\n\nclass BCETrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n\n        loss_fct = torch.nn.BCEWithLogitsLoss()\n        loss = loss_fct(logits.view(-1), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\n\ndef compute_roc_auc(eval_pred):\n    # logits, labels = eval_pred.predictions, eval_pred.label_ids\n    logits, labels = eval_pred\n    labels = labels.astype(int)\n    if labels.std() < 1E-8:  # only one class present in dataset\n        return {\"roc_auc\": 0.0}\n\n    ps = expit(logits).reshape(-1)\n    return {\"roc_auc\": roc_auc_score(labels, ps)}\n\n\ndef main(cfg, save_dir, model_id):\n    accelerator = Accelerator()\n\n    # data ----\n    rng = random.Random(cfg.seed)\n    \n    try:\n        essay_df = pd.read_csv(cfg.train_data_path)\n    except Exception as e:\n        essay_df = pd.read_parquet(cfg.train_data_path)\n        \n    essay_df = essay_df.rename(columns={\"generated\": \"label\"})\n    print(essay_df.shape)\n    \n    N = int(cfg.train_frac * len(essay_df))\n    essay_df = essay_df.sample(N).reset_index(drop=True)\n\n    essay_df['fold'] = essay_df['text'].apply(\n        lambda x: 'train' if rng.random() < 0.99 else 'valid'\n    )\n    \n    train_df = essay_df[essay_df['fold'] == 'train'].copy()\n    valid_df = essay_df[essay_df['fold'] == 'valid'].copy()\n    valid_df = valid_df.sample(min(1000, len(valid_df))).reset_index(drop=True)\n\n    train_df = train_df.reset_index(drop=True)\n    valid_df = valid_df.reset_index(drop=True)\n    \n    \n    \n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        test_df = pd.read_csv(\"/kaggle/input/textdata/test_essays.csv\", sep=',')\n    else:\n        test_df = pd.read_csv(\"/kaggle/input/textdata/test.csv\", sep=',')\n        \n    ##########################################################################################\n    # PL ---\n    accelerator.print(\"##\" + \"~~\"*40 + \"##\")\n    pl_df = deepcopy(test_df)\n    \n    score_dfs = []\n    \n    for sp in cfg.pl_score_paths:\n        score_dfs.append(pd.read_parquet(sp).rename(columns={\"generated\": \"label\"}))\n    scores_df = pd.concat(score_dfs).reset_index(drop=True)\n    scores_df = scores_df.groupby(\"id\")[\"label\"].mean().reset_index()\n    \n    accelerator.print(scores_df.head())\n    #-----\n    \n    pl_df = pd.merge(pl_df, scores_df, on='id', how='inner')\n    accelerator.print(f\"Shape of PL: {pl_df.shape}\")\n    \n    accelerator.print(f\"Shape of train: {train_df.shape}\")\n    train_df =  train_df[['id', 'text', 'label']].copy()\n    pl_df = pl_df[['id', 'text', 'label']].copy()\n    train_df = pd.concat([train_df, pl_df, pl_df, pl_df]).reset_index(drop=True) # 4x PL oversample\n    accelerator.print(f\"Shape of train after merging PL: {train_df.shape}\")\n    accelerator.print(\"##\" + \"~~\"*40 + \"##\")\n    ##########################################################################################\n\n    if cfg.debug:\n        train_df = train_df.sample(1000, random_state=cfg.seed).reset_index(drop=True)\n        valid_df = valid_df.sample(128, random_state=cfg.seed).reset_index(drop=True)\n\n    accelerator.print(\"##\" + \"~~\"*40 + \"##\")\n    accelerator.print(f\"Train df shape: {train_df.shape}\")\n    accelerator.print(f\"Valid df shape: {valid_df.shape}\")\n    accelerator.print(f\"Test df shape: {test_df.shape}\")\n    accelerator.print(\"##\" + \"~~\"*40 + \"##\")\n\n    # tokenizer ---\n\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model_dir)\n\n    # dataset ---\n    with accelerator.main_process_first():\n        dataset_creator_train = AiDataset(tokenizer, cfg.max_length_train)\n        dataset_creator_infer = AiDataset(tokenizer, cfg.max_length_infer)\n\n        train_ds = dataset_creator_train.get_dataset(train_df)\n        valid_ds = dataset_creator_train.get_dataset(valid_df)\n        test_ds = dataset_creator_infer.get_dataset(test_df)\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n\n    for idx in range(4):\n        accelerator.print(f\"\\n--- Sample {idx} ---\\n\")\n        accelerator.print(repr(tokenizer.decode(train_ds[idx][\"input_ids\"])))\n        accelerator.print(\"##\" + \"~~\"*40 + \"##\")\n\n    model = DebertaV2ForSequenceClassification.from_pretrained(cfg.model_dir, num_labels=1)\n\n    # training args ---\n    training_args = TrainingArguments(\n        output_dir=cfg.model_dir,\n        num_train_epochs=1,\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        gradient_accumulation_steps=2,\n        learning_rate=5e-5,\n        warmup_steps=0.05,\n        weight_decay=0.01,\n        logging_dir=\"logs\",\n        logging_steps=50,\n        report_to=\"none\",\n        evaluation_strategy=\"steps\",\n        eval_steps=500, # 250\n        metric_for_best_model=\"roc_auc\",\n        greater_is_better=True,\n        max_grad_norm=1.0,\n        optim=\"adamw_bnb_8bit\"\n    )\n\n    trainer = BCETrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds,\n        eval_dataset=valid_ds,\n        compute_metrics=compute_roc_auc,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n\n    # prediction ---\n    pred_output = trainer.predict(test_ds)\n    logits = pred_output.predictions.astype(float)\n    probs = expit(logits).reshape(-1)\n\n    sub_df = pd.DataFrame({\n        \"id\": test_ds['id'],\n        \"generated\": probs\n    })\n\n    if accelerator.is_main_process:\n        save_path = os.path.join(save_dir, f\"{model_id}.parquet\")\n        sub_df.to_parquet(save_path)\n        accelerator.print(\"done!\")\n        accelerator.print(\"~~\"*40)\n\n\nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument('--config_path', type=str, required=True)\n    ap.add_argument('--save_dir', type=str, required=True)\n    ap.add_argument('--model_id', type=str, required=True)\n\n    args = ap.parse_args()\n    cfg = OmegaConf.load(args.config_path)\n\n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        print(\"Setting DEBUG to False\")\n        cfg.debug = False\n\n    os.makedirs(args.save_dir, exist_ok=True)\n    os.makedirs(cfg.model_dir, exist_ok=True)\n\n    # execution\n    main(\n        cfg,\n        save_dir=args.save_dir,\n        model_id=args.model_id,\n    )\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:31:02.030467Z","iopub.execute_input":"2024-04-11T10:31:02.030789Z","iopub.status.idle":"2024-04-11T10:31:02.041540Z","shell.execute_reply.started":"2024-04-11T10:31:02.030764Z","shell.execute_reply":"2024-04-11T10:31:02.040646Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Writing run_train_infer_deberta.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile ./configs/conf_deberta_mlm.yaml\n\nseed: 42\ndebug: true\n    \ntrain_data_path: /kaggle/input/persaude-corpus-2/persuade_2.0_human_scores_demo_id_github.csv\n\nmax_length: 1024\nvocab_size: 4096\ntrain_frac_for_tokenizer: 0.5\ntrain_frac_for_mlm: 0.8\nmask_probability: 0.20\n\nlr: 4e-5\nper_device_batch_size: 4\ngradient_accumulation_steps: 4\nweight_decay: 0.01\n\nnum_train_epochs: 4\nwarmup_pct: 0.05\ntest_pct: 0.005\nmax_train_examples: 100_000\neval_frequency: 100 # 1024\nmax_grad_norm: 1.0\n\nmodel_dir: \"./models/deberta_v3_small_persuade\"\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:31:06.401560Z","iopub.execute_input":"2024-04-11T10:31:06.401962Z","iopub.status.idle":"2024-04-11T10:31:06.408034Z","shell.execute_reply.started":"2024-04-11T10:31:06.401931Z","shell.execute_reply":"2024-04-11T10:31:06.407072Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Writing ./configs/conf_deberta_mlm.yaml\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile ./configs/conf_deberta.yaml\n\nseed: 42\ndebug: true\ntrain_data_path: /kaggle/input/traindeberta/train_essays.csv\n    \ntrain_frac: 0.5\nmax_length_train: 512\nmax_length_infer: 1296\n    \npl_score_paths:\n    - ./outputs/m0.parquet\n    - ./outputs/m1.parquet\nmodel_dir: \"./models/deberta_v3_small_persuade\"","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:31:09.805172Z","iopub.execute_input":"2024-04-11T10:31:09.806105Z","iopub.status.idle":"2024-04-11T10:31:09.811961Z","shell.execute_reply.started":"2024-04-11T10:31:09.806066Z","shell.execute_reply":"2024-04-11T10:31:09.810811Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Writing ./configs/conf_deberta.yaml\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 run_deberta_mlm.py \\\n--config_path \"./configs/conf_deberta_mlm.yaml\"","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:31:21.609815Z","iopub.execute_input":"2024-04-11T10:31:21.610177Z","iopub.status.idle":"2024-04-11T10:33:58.231144Z","shell.execute_reply.started":"2024-04-11T10:31:21.610147Z","shell.execute_reply":"2024-04-11T10:33:58.229930Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"2024-04-11 10:31:32.707749: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 10:31:32.707751: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 10:31:32.707808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 10:31:32.707817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 10:31:32.709899: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-04-11 10:31:32.709899: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\nsetting seed: 42\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\nshape of input text data: (1300, 2)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\nTrain tokenizer with 500 train df samples\n\u001b[2K[00:00:00] Tokenize words                 ██████████████████ 12781    /    12781[00:00:00] Tokenize words                 ██████████████████ 0        /        0\n\u001b[2K[00:00:00] Tokenize words                 ██████████████████ 12781    /    12781\n\u001b[2K[00:00:00] Count pairs                    ██████████████████ 12781    /    12781\n\u001b[2K[00:00:00] Count pairs                    ██████████████████ 12781    /    12781\n\u001b[2K[00:00:00] Compute merges                 ██████████████████ 3985     /     3985\n\u001b[2K[00:00:00] Compute merges                 ██████████████████ 3985     /     3985\n  0%|                                                     | 0/2 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1157 > 1024). Running this sequence through the model will result in indexing errors\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.65ba/s]\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.10ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 26.46ba/s]\n  0%|                                                     | 0/2 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1157 > 1024). Running this sequence through the model will result in indexing errors\n 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.14s/ba]using bnb optimizer....\n================================================================================\ninput text:\n , ĠI Ġwas Ġcontin ually Ġste pping Ġoutside Ġmy Ġcomfort Ġzone Ġin Ġthe Ġmost Ġreward ing Ġof Ġways [MASK] ĠI Ġalso Ġg re w Ġto Ġapp [MASK] [MASK] [MASK] Ġthe Ġshe er Ġbeaut [MASK] Ġof Ġco ast l ines, Ġoceans Ġand Ġmar ine Ġlife Ġin [MASK] [MASK] [MASK] [MASK] [MASK] Ġbe Ġachieve d Ġby Ġsimply Ġlooking Ġat Ġpictures [MASK] [MASK] [MASK] [MASK] iving Ġand Ġworking Ġab o ard Ġthe Ġboat Ġfor Ġan Ġext ended Ġperiod Ġwas Ġno Ġeasy Ġtask, Ġbut Ġit [MASK] [MASK] [MASK] [MASK] [MASK] ed Ġme Ġphysically Ġin Ġways Ġthat Ġhave Ġstay ed Ġwith Ġme. ĠThere Ġwere Ġmany Ġlong Ġdays Ġof Ġdifficult Ġman ual Ġl ab or Ġto Ġmaintain Ġthe Ġv ess el Ġand Ġcare Ġfor Ġthe Ġcattle. ĠHowever, Ġthe Ġsense Ġof Ġpri de [MASK] [MASK] [MASK] [MASK] ment ĠI Ġfelt Ġfrom Ġf acing Ġchallenges Ġhigh Ġon Ġthe Ġopen Ġocean Ġwas Ġincred ib ly Ġpowerful. ĠI Ġalso Ġgained Ġvaluable Ġskills Ġsuch Ġas Ġnavig ation, Ġweather Ġobser v ation, Ġhe av [MASK] Ġlif ting Ġand [MASK] [MASK] [MASK] air s Ġthat Ġwill Ġser ve Ġme Ġwell Ġthroughout Ġmy Ġcareer. Ġ Ċ Ċ If Ġyou Ġwant Ġto Ġexperience Ġthe Ġth ri [MASK] Ġof Ġadventure [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] t able Ġplaces, Ġexpand ing Ġyour Ġworld v iew Ġand Ġdeveloping Ġimportant Ġlife Ġskills, Ġthen ĠI Ġcannot Ġrec omm end Ġthe ĠSeagoing ĠCowboys Ġprogram Ġhighly Ġenough. ĠBy [MASK] [MASK] [MASK] Ġthis Ġonce - in - a - l if etime Ġopportunity, Ġyou Ġwill Ġret urn Ġhome Ġa Ġchanged Ġperson [MASK] Ġa Ġlifetime Ġof Ġim m ers ive Ġmem ories Ġto Ġlook Ġback Ġon Ġf ond ly. ĠSo Ġwhat Ġare Ġyou [MASK] [MASK] [MASK] [MASK] ign Ġup Ġtoday Ġand Ġlet Ġthe Ġseas Ġchange Ġyour Ġlife Ġas Ġthey Ġdid Ġm [MASK] [MASK] [SEP] [CLS] [MASK] [MASK] [MASK] Ġprogram Ġis Ġan Ġincredible [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġthe Ġworld, Ġlearn Ġnew Ġskills, Ġand Ġmake [MASK] [MASK] [MASK] [MASK] [MASK] Ġof Ġothers. ĠI Ġbelieve Ġthat Ġthis Ġprogram Ġis Ġan Ġamazing Ġway Ġto Ġbroad en [MASK] [MASK] [MASK] [MASK] iew Ġand Ġgain Ġvaluable Ġexperiences [MASK] [MASK] [MASK] Ġwould Ġlove Ġto Ġjoin Ġits Ġr an [MASK] [MASK] [MASK] [MASK] [MASK] ĠSeagoing ĠCowboys Ġprogram Ġoff ers Ġa Ġunique Ġadventure Ġthat [MASK] [MASK] [MASK] [MASK] Ġelse. ĠImagine Ġsa iling [MASK] [MASK] Ġseas, Ġcar ing Ġfor Ġhorses [MASK] [MASK] [MASK] [MASK] Ġdifferent Ġcultures Ġin Ġvarious Ġcountries. ĠLuke's Ġstory Ġis Ġa Ġpri me Ġexample Ġof Ġthe Ġexciting Ġj our ne ys [MASK] Ġare Ġpossible Ġthrough Ġthis Ġprogram. ĠHe Ġtra veled Ġto ĠGreece, ĠChina, Ġand [MASK] [MASK], Ġexp osing Ġhim self Ġto Ġd iver se Ġc ust om s Ġand Ġways Ġof Ġlife. ĠAt Ġjust Ġ18 Ġyears Ġold, Ġhe Ġexperienced Ġthings Ġthat Ġmost Ġpeople Ġcan Ġonly Ġdre am Ġof, [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ing. Ċ Ċ Furthermore, Ġthe ĠSeagoing ĠCowboys Ġprogram Ġnot Ġonly [MASK] Ġan Ġadventure Ġbut Ġalso Ġteach es Ġvaluable [MASK]. ĠAs Ġa Ġseagoing Ġcowboy, Ġone Ġlearn s Ġhow Ġto Ġcare Ġfor [MASK] [MASK] Ġnavig ate Ġthrough Ġ rough Ġseas, Ġand Ġwork Ġin Ġa Ġteam Ġenvironment. ĠLuke's Ġexperience Ġof Ġcar ing Ġfor Ġ3 3 5 Ġhorses Ġon Ġhis Ġfirst Ġmission Ġis Ġa Ġtest am ent Ġto Ġthe Ġprogram's Ġability Ġto Ġbuild [MASK] id ence Ġand Ġresponsibility. ĠAdditionally [MASK] [MASK] [MASK]'s Ġfocus Ġon Ġhuman it ar ian Ġa id Ġand Ġintern ational Ġco op er [MASK] Ġprom ot es Ġem p ath y Ġand Ġunderstanding Ġamong Ġdifferent Ġn ations. Ċ Ċ M ore over, Ġthe ĠSeagoing ĠCowboys Ġprogram Ġis Ġan Ġopportunity Ġto [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] iver ing Ġanimals Ġto Ġcountries Ġin Ġneed, Ġparticip [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] - be [MASK] Ġof Ġcommunities Ġthat Ġhave Ġbeen Ġaffected Ġby Ġconf lic t Ġor [MASK] [MASK] ast ers. ĠLuke [MASK] [MASK] [MASK] [MASK] [MASK] ing Ġanimals Ġto Ġwar [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġhe art w arm ing Ġexample Ġof Ġhow Ġthis Ġprogram Ġcan Ġchange Ġlives Ġfor Ġthe Ġbetter. Ċ Ċ In Ġconclusion, Ġthe [MASK] [MASK] [MASK] [MASK] Ġan Ġextra ord in ary Ġopportunity Ġfor Ġyoung Ġpeople Ġto Ġexplore Ġthe Ġworld, Ġgain Ġvaluable Ġskills, [MASK] [MASK] [MASK] [MASK] Ġin Ġthe Ġlives [MASK] [MASK] [MASK] [MASK] [MASK] [MASK], Ġlearning, Ġand Ġhuman it ar ian Ġa id, Ġthis Ġprogram Ġis [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġto Ġbroad en Ġone's Ġhor iz ons Ġand Ġgrow Ġas Ġa Ġperson. ĠLuke's Ġexperience Ġis Ġa Ġcomp elling Ġexample Ġof Ġthe Ġprogram's Ġbenefits, Ġand ĠI Ġbelieve Ġthat Ġjoining Ġthe ĠSeagoing ĠCowboys Ġprogram Ġwould Ġbe Ġan Ġincredible Ġj [MASK] [MASK] [MASK] Ġwould Ġshape [MASK] [MASK] Ġa Ġmore Ġinformed, Ġcomp ass ion ate, Ġand Ġadvent ur ous Ġindividual. [SEP] [CLS] ĠThere Ġare Ġmany Ġreasons Ġto Ġjoin Ġthe Ġprog [MASK] [MASK] [MASK] [MASK] Ġof Ġthe Ġreasons Ġthere Ġis Ġthe Ġway. ĠThe Ġstar Ġprovides Ġneeded Ġar [MASK] [MASK] Ġfly [MASK] [MASK] Ġsa iling Ġfle et Ġwhen Ġon Ġbe [MASK] [MASK] [MASK] [MASK] ĠStates [MASK] Ġgoal Ġcame Ġin Ġ200 2, [MASK] [MASK] Ġwanted Ġas Ġa Ġcountry Ġthe Ġc ow Ġgu y Ġboat s Ġto Ġmeet Ġup Ġthe Ġhuman Ġpassengers Ġand [MASK] Ġlead ers Ġof [MASK] [MASK] [MASK] Ġwar Ġwe ary Ġc ow Ġanimals Ġhe [MASK] [MASK] [MASK] [MASK] Ġtheir Ġj our ney Ġthrough Ġan Ġunc o's Ġtrue Ġand Ġrem [MASK] [MASK] [MASK] [MASK] [MASK] ric a The Ġc ow Ġnumber Ġ7 4 s Ġits Ġbe Ġback Ġonly Ġ7 9 4 Ġgoing Ġpeople Ġmight Ġsee Ġy aho o Ġbut Ġthat Ġstarts [MASK] [MASK] [MASK] [MASK] [MASK] Ġmore Ġdangerous Ġthey Ġare Ġhe ats ong [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġg un s Ġand Ġmost Ġw ars Ġdon 't Ġlong Ġend Ġwhen Ġsomething Ġto Ġfind Ġand Ġmake Ġwhat Ġhappen. [MASK] Ġget Ġto Ġa Ġspot Ġand Ġsolve Ġthe [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġ194 2 A Ġ vers ion Ġonly Ġhad Ġ2 Ġg ang [MASK] [SEP] [CLS] ĠH i Ġim ĠLuke Ġim Ġgoing Ġto Ġtell Ġyou\n================================================================================\ninput text:\n Ġsubject Ġis Ġa Ġgood Ġidea Ġ& Ġyou Ġcould [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġor Ġbad Ġday Ġat Ġschool Ġwhich Ġcould Ġsave Ġlives Ġin Ġthe Ġvery Ġlong Ġrun Ġfinally, Ġyou Ġcould Ġtell Ġif Ġthe Ġstudent Ġis Ġbeing Ġstressed Ġout Ġor Ġjust Ġworried Ġbefore Ġthe [MASK] [MASK] Ġthe Ġtest [MASK] [MASK] [MASK] Ġthink Ġthat Ġthis Ġwould Ġbe Ġa Ġgreat Ġidea Ġto Ġhave Ġin [MASK] [MASK] [MASK] [MASK] ĠUnited ĠStates Ġand Ġthe ĠN ation [MASK] [MASK] ĠĠĠĠĠĠ [SEP] [CLS] ĠMany Ġstudents Ġhave Ġthe Ġoption Ġto Ġattend Ġonline Ġschool Ġin Ġthe Ġcomfort Ġof Ġtheir Ġown Ġhomes. ĠFor Ġparents, Ġthe Ġmatter Ġof Ġletting Ġtheir Ġchild Ġbe Ġsch o o [MASK] Ġat Ġhome Ġis Ġan Ġimportant Ġchoice Ġthat Ġmay Ġinfluence Ġwho Ġtheir Ġchild Ġgrow s Ġup Ġto Ġbe. ĠIt's Ġa Ġdifficult [MASK] [MASK] [MASK] [MASK] [MASK] Ġproblem Ġis Ġdeciding Ġwhether Ġor Ġnot Ġonline Ġschooling Ġis Ġbeneficial Ġto Ġstudents. ĠStudent's Ġwouldn [MASK] [MASK] [MASK] [MASK] [MASK] Ġat Ġhome Ġbecause Ġwork Ġwouldn 't Ġget Ġdone Ġand Ġthey Ġwouldn 't Ġbe Ġable Ġto Ġbe Ġtaught Ġas Ġefficient ly [MASK] [MASK] [MASK] C l ass work Ġis Ġan Ġimportant Ġpart Ġof Ġschool Ġthat Ġallows Ġteachers [MASK] Ġservice [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġwas Ġtaught Ġto Ġthem. ĠIt Ġalso Ġgives Ġstudents Ġthe Ġopportunity Ġto Ġpractice Ġand Ġstudy. ĠSome Ġstudents, Ġhowever [MASK] [MASK] [MASK] [MASK] Ġdoing Ġclass work Ġand Ġwill Ġprocrast inate Ġon Ġprojects Ġor Ġwon 't Ġdo [MASK] [MASK] [MASK] [MASK] ĠThis [MASK] Ġget Ġworse Ġof [MASK] [MASK] Ġonline Ġschool. ĠAt Ġhome, Ġthere Ġmay Ġbe Ġno Ġone Ġto Ġkeep Ġthem [MASK] [MASK] [MASK] [MASK] [MASK] Ġneed Ġto Ġwork Ġduring Ġschool Ġhours Ġand Ġwon 't Ġbe Ġthere Ġto Ġhelp Ġtheir Ġchild Ġkeep Ġup Ġwith Ġtheir Ġschool work. ĠAt t ending [MASK] [MASK] [MASK] [MASK] [MASK] Ġalways Ġa Ġteacher [MASK] Ġthe Ġclassroom Ġthat Ġkeeps Ġthe Ġstudents Ġdoing Ġtheir Ġwork, Ġand Ġwill Ġcorrect Ġthem Ġif Ġthe Ġstudents Ġappear Ġto Ġbe Ġgoing Ġoff Ġtask. Ċ Ċ Furthermore, Ġthe Ġphysical Ġaspect Ġof Ġteaching Ġwill Ġbe Ġgone Ġif Ġstudents [MASK] Ġschool Ġat Ġhome. ĠD Ġpursu [MASK] [MASK] [MASK] [MASK] [MASK] Ġa Ġch em ist [MASK] [MASK] [MASK] [MASK] [MASK] Ġbecause Ġof Ġthe [MASK] [MASK] [MASK] [MASK] [MASK] Ġor Ġa Ġspecific Ġenvironment. ĠUsing ĠB un s en Ġburn [MASK] Ġfor [MASK] Ġassignment [MASK] [MASK] [MASK] [MASK] Ġthe Ġhouse Ġwithout Ġadult Ġsup erv ision. ĠOnline Ġschools Ġmay Ġnot Ġbe Ġable Ġto Ġoffer Ġa Ġphysical [MASK] [MASK], Ġso Ġteachers Ġcan 't Ġmake Ġsure Ġthat Ġchildren Ġare Ġgetting Ġthe Ġamount Ġof Ġphysical Ġactivity [MASK] [MASK] [MASK] [MASK] [MASK] Ġsome Ġstudents Ġmay Ġneed Ġa Ġfun [MASK] Ġhands Ġon Ġapproach Ġto Ġlearn Ġthe Ġmaterial [MASK] ly. ĠHaving [MASK] Ġteacher Ġand [MASK] Ġchildren Ġaround Ġthem Ġimpro ves Ġa Ġstudent's Ġlearning Ġexperience [MASK] [MASK] Ġrequired [MASK] [MASK] [MASK] [MASK] Ġclasses Ġat Ġhome Ġwould Ġprovide Ġlittle Ġgrowth [MASK] [MASK] [MASK]'s Âł so cial Ġlife. ĠPeople Ġwho [MASK] [MASK] [MASK] [MASK] [MASK] ing Ġwould Ġsay Ġthat Ġchildren Ġparticipate Ġin Ġafter Ġschool Ġactivities Ġand Ġmeet [MASK] [MASK] Ġconstantly. ĠHowever, Ġthere Ġare Ġmany Ġstudents Ġwho Ġare Ġtoo Ġanx ious, Ġintro ver ted, [MASK] Ġun m ot iv ated Ġto Ġengage Ġin Ġother Ġactivities. ĠFor Ġsome, Ġthere Ġis Ġonly Ġone Ġplace Ġwhere Ġthey Ġsocial ize Ġwith Ġothers : Ġschool. ĠIn Ġthe Ġclassroom, Ġchildren Ġare Ġalways Ġsurround [MASK] Ġby Ġother Ġstudents Ġto Ġgrow Ġwith Ġand [MASK] [MASK] ĠNot Ġonly Ġare Ġfriends hips Ġformed Ġat Ġschool, Ġbut Ġthere Ġare Ġalso Ġmany Ġcouple s Ġseen Ġthroughout [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK], Ġattending Ġclasses Ġat Ġhome Ġwouldn 't Ġbe Ġas Ġbeneficial [MASK] [MASK] [MASK] Ġof Ġthree Ġthings : Ġmore Ġstudents Ġwouldn 't Ġget [MASK] [MASK] [MASK] [MASK] Ġteaching Ġwouldn 't Ġbe Ġas Ġefficient Ġfor Ġstudents ; Ġfurther more, Ġthe Ġsocial Ġlives Ġof Ġthe Ġstudents Ġwould Ġsuffer. ĠIf [MASK] [MASK] [MASK] Ġreasons Ġwhy Ġa Ġparticular Ġstudent Ġwouldn 't Ġwant Ġto Ġattend Ġschool, Ġthey Ġshould Ġgo Ġto Ġthe Ġprinciple Ġor Ġguid ance Ġcoun sel or [MASK] [MASK] [MASK] [MASK] [MASK] Ġtry Ġto Ġfind Ġa Ġsolution Ġto Ġtheir Ġproblems, Ġor Ġfind Ġa Ġway Ġfor Ġthe Ġschool Ġto Ġacc omm od ate Ġtheir Ġneeds. [SEP] [CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġshould Ġnot Ġhave Ġto Ġdo Ġany Ġcommunity [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġkids Ġhave Ġother Ġthings Ġthat Ġthey Ġneed Ġto Ġworry Ġabout. ĠFor Ġexample, Ġteachers Ġgive Ġkids Ġa Ġt on Ġof Ġhomework Ġeveryday. ĠTo Ġbe Ġable Ġto Ġdo Ġthat Ġhomework, Ġwe Ġneed Ġtime. ĠCommunity ĠService Ġwould Ġtake Ġaway Ġthat Ġtime Ġto Ġdo Ġhomework Ġand Ġstudy Ġfor Ġtests. Ċ Ċ [MASK] [MASK] Ġwhy Ġwe Ġshouldn 't Ġhave Ġto Ġdo Ġcommunity Ġservice Ġis Ġbecause Ġof Ġextra Ġcur ricular Ġactivities. ĠKids Ġplay Ġall Ġtypes Ġof Ġsports. ĠThey Ġhave [MASK] [MASK] [MASK] Ġweek ends Ġand Ġon Ġthe Ġweek Ġdays. ĠThey Ġalso Ġhave Ġto Ġfit Ġin Ġthe Ġhomework Ġfactor. ĠThere Ġis Ġnot Ġenough [MASK] Ġin Ġthe Ġday Ġto Ġfit Ġin Ġschool, Ġhomework, Ġextra Ġcur ricular Ġactivities, Ġand Ġcommunity Ġservice. ĠWe Ġcould Ġdo Ġit Ġif Ġwe Ġwould Ġwant Ġto, Ġbut Ġwe Ġshould Ġnot Ġhave Ġto. ĠMany Ġkids Ġdon 't Ġhave Ġthe Ġtime Ġto Ġfit Ġin [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġactivities Ġand Ġhomework, [MASK] [MASK] [MASK] [MASK] [MASK] Ċ Ċ [MASK] [MASK] Ġhave Ġmany Ġchores Ġto Ġdo Ġat Ġhome. ĠThis Ġalso Ġtakes Ġtime Ġaway Ġto Ġdo [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġthings Ġkids Ġmight Ġdo. ĠD ep ending Ġon Ġthe Ġch ore Ġdepend s Ġon Ġhow Ġmuch Ġtime Ġit Ġtakes Ġto Ġthat Ġparticular Ġch ore. Ċ Ċ [MASK] [MASK] [MASK] [MASK] Ġalso Ġtake Ġtime Ġaway [MASK] Ġbeing [MASK] [MASK] [MASK] [MASK] cial iz ing Ġis Ġa Ġbig Ġpart [MASK] [MASK] [MASK] [MASK] [MASK] Ċ Ċ They Ġneed Ġto Ġbe Ġsocial. ĠThey Ġalso Ġhave Ġto Ġbe Ġactive [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġparticipate Ġin Ġa Ġsport, Ġthey Ġmight Ġbe Ġin Ġa [MASK] [MASK] Ġthey Ġcannot Ġtake Ġthe Ġtime Ġto Ġdo Ġany ĠCommunity ĠService. Ċ Ċ All Ġin Ġall, ĠI Ġdo Ġnot Ġthink\n================================================================================\ninput text:\n Ġwhy Ġyou Ġshould Ġjoin Ġthe Ċ Ċ Seagoing ĠCowboys Ġprogram. ĠIf Ġyu Ġhave Ġany Ġquestions Ġi Ġhave Ġyu Ġcould Ġask Ġthem. ĠIt [MASK] [MASK] Ġu Ġinvolved Ġin Ġplaces Ġvery Ġra re Ġbut Ġyou [MASK] [MASK] Ġto Ġtell Ġanybody Ġwhy Ġu Ġshould Ġjoin Ġthis Ġprogram. ĠI Ġwant Ġto Ġtell Ġyou Ġwhy Ġyou Ġshould [MASK] Ġthis Ġprogram Ġbecause Ġthen Ġyou Ġwill Ġbe Ġable Ġto Ċ Ċ f ind Ġmore Ġfun [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġhave Ġmy Ġreasons Ġwhy Ġyou Ġshould Ġjoin Ġthe ĠSeagoing ĠCowboys Ġprogram Ġtoo, Ġthat Ġyou Ġw ll Ġlearn Ġto Ġcare Ġfor Ġyour Ġanimals [MASK] Ġpeople Ġout Ġin Ġplaces Ġvery Ġra re Ġor Ġra re. Ċ Ċ First ĠIm Ġgonna Ġtell [MASK] Ġto Ġgo Ġbe Ġin Ġthe Ġregion Ġwhere Ġyou Ġshould Ġbe Ġin Ġa Ġplace Ġlong [MASK] [MASK] Ġthat Ġthat Ġsounds Ġinteresting Ġbut Ġno Ġone Ġare Ġgoing Ġto Ġjoin Ġyou Ġif [MASK] [MASK] Ġwanna Ġbe Ġin Ġthe Ġregion. ĠAny way ĠLuke Ġi Ġknow Ġthis Ġsounds [MASK] [MASK] [MASK] Ġone [MASK] Ġgoing Ġto Ġjoin Ġyou Ġif Ġyou Ġdon 't Ġlike Ġthe ĠSeagoing ĠCowboys ĠPro gram Ġthen Ġyou Ġmay Ġnot Ġcome Ġto Ġthe Ġregion [MASK] [MASK] Ċ L ast ĠIm [MASK] Ġtell [MASK] [MASK] Ġthe Ġadventure Ġin Ġthe Ġregion. ĠIf Ġyu Ġhave Ġany Ġquestions Ġhow Ġshould [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġyou Ġdo Ġyu Ġjob? ĠWhat Ġshould Ġu Ġdo Ġthe Ġjob? ĠWhat Ġshould Ġu Ġdo Ġwith Ġall Ġthe Ġanimals? ĠLuke Ġi Ġknow Ġyou Ġwant Ġto Ġtell Ġus [MASK] [MASK] Ġyour Ġhelp Ġin Ġthe Ġregion Ġbut Ġthis Ġis Ġjust Ġyu Ġreasons. [MASK] Ġfor Ġhow Ġto Ġdo Ġyu Ġjob Ġi Ġdont Ġknow Ġwhy Ġyou Ġjust Ġleft Ġa Ġcom ment Ġhere Ġsaying Ġit Ġdoesnt Ġmatter Ġbecuase [MASK] [MASK] [MASK] Ġleft Ġit Ġbefore Ġbut Ġim [MASK] Ġgoing Ġto Ġtell Ġyu Ġall. [MASK] [MASK] [MASK] [MASK] Ġwant Ġto Ġjoin Ġyu Ġshould Ġdo Ġit, Ġit [MASK] [MASK] [MASK] [MASK] [MASK] Ġto Ġdo Ġyu Ġjob Ġbecause Ġit Ġwill Ġget Ġyou Ġhelp Ġand Ġhelp Ġout Ġyour Ġneighbor Ġwhen, Ġit Ġwill Ġget Ġyou Ġjob Ġand Ġhelp Ġyour Ġneigh b our [MASK] Ġthe Ġway. [SEP] [CLS] ĠLuke ĠBomberger [MASK] Ġa Ġvery Ġsmart Ġperson. ĠHe Ġstarted Ġworking Ġat Ġhis Ġa un t [MASK] Ġr an ch, [MASK] [MASK] [MASK] Ġthat Ġwouldn 't [MASK] [MASK] ver. ĠLuke Ġdecided Ġto [MASK] [MASK] [MASK] [MASK] [MASK]. ĠThis Ġwas Ġa Ġgreat Ġopportunity Ġfor Ġhim Ġand Ġother Ġpeople Ġwho Ġjoined. ĠIt Ġwas Ġan Ġexperience Ġof Ġa Ġlifetime [MASK] Ġchanged Ġmany Ġpeoples Ġlives. ĠYou Ġshould Ġalso Ġjoin Ġthe ĠSeagoing ĠCowboys ĠPro gram Ġtoo Ġbecause Ġyou Ġcan Ġhelp Ġmany Ġcountries, Ġvisit [MASK] [MASK] [MASK] sit iful, Ġand Ġhave Ġfun Ġwhile Ġdoing Ġit. Ċ Ċ You Ġcould Ġchange Ġmany Ġpeoples Ġlives Ġby Ġjoining [MASK] [MASK] [MASK] [MASK] [MASK], ĠWorld ĠWar ĠII Ġhad Ġjust Ġend ed Ġin ĠEurope Ġand Ġmany Ġcountries Ġwere Ġleft Ġin Ġru ins. ĠTo Ġrec over Ġfrom Ġall Ġof Ġthe [MASK] Ġday [MASK], Ġthe ĠUNRRA Ġh ired [MASK] Seagoing ĠCowboys \" Ġto Ġtake Ġcare Ġof Ġhorses, Ġyoung Ġc ows, Ġmu les, Ġand Ġdon ke ys Ġover Ġseas [MASK] [MASK] Ġjoining Ġthese [MASK] [MASK] [MASK] [MASK] [MASK] Ġcountries Ġget Ġback Ġon Ġtheir [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġlike Ġfood, Ġwater, Ġmed ic ine [MASK] [MASK] [MASK] [MASK], Ġand Ġmore. ĠAs ĠLuke [MASK] [MASK] [MASK] [MASK] bo at Ġtrips Ġwere Ġunb elie [MASK] [MASK] [MASK] [MASK] Ġa Ġsmall t own Ġboy.\" ĠBeing Ġpart Ġof Ġthe ĠSeagoing ĠCowboys Ġwas Ġmuch Ġmore Ġthan Ġan Ġadventure Ġto Ġme. ĠI Ġam Ġgr [MASK] [MASK] Ġfor Ġbeing Ġable Ġto Ġdo Ġso. Ċ Ċ We Ġgot Ġto Ġsee Ġsome Ġamazing Ġs ights Ġduring Ġour Ġtravel [MASK] [MASK] Ġoceans [MASK] [MASK] [MASK] Ġwe Ġwent Ġs ight se eing Ġwhen Ġwe [MASK] [MASK] [MASK] Ġat ĠGreece. ĠThe ĠAc rop [MASK] [MASK] [MASK] [MASK] Ġof [MASK] [MASK] [MASK] Ġthings Ġwe Ġsaw Ġthere. ĠWe [MASK] [MASK] [MASK] Ġg ond ol a Ġride Ġin ĠVen ice, ĠIt aly. ĠA [MASK] [MASK] Ġstreets Ġof Ġwater! ĠThat Ġwas Ġpretty Ġcool [MASK] [MASK] [MASK] ing Ġa Ġcastle Ġin ĠC rete [MASK] [MASK] [MASK] [MASK] Ġto Ġplay Ġtable Ġten nis, Ġbaseball, [MASK] [MASK] [MASK] [MASK] [MASK], Ġf encing, Ġreading, Ġwriting, Ġwh itt [MASK] [MASK] [MASK] [MASK] Ġmore Ġgames Ġto Ġpass Ġthe Ġlong Ġj our ney Ġhome. ĠEven Ġthough Ġtaking Ġcare Ġof [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ic Ġloc ations Ġsound ed Ġboring, Ġyou Ġalways Ġfound Ġsomething Ġto Ġdo Ġalong Ġthe Ġway. ĠThere Ġwasn 't Ġa Ġd ull Ġmoment. Ċ Ċ In Ġconclusion, ĠI Ġthink Ġeveryone Ġwould Ġenjoy Ġbeing Ġapart Ġof Ġthe ĠSeagoing ĠCowboys Ġprogram. ĠEvery one Ġshould Ġbe Ġa Ġgood Ġs am ar it an, Ġnot [MASK] Ġby Ġhelping Ġothers, Ġbut Ġby Ġsh aring Ġyour Ġwe alth Ġas Ġwell. ĠIf Ġyou Ġwant [MASK] [MASK] Ġa Ġdifference Ġin Ġthe Ġworld, Ġthen Ġgo Ġout Ġthere Ġand [MASK] [MASK] [MASK] [MASK]'s Ġlife Ġtoday. ĠJust Ġremember, [MASK] [MASK] Ġneed [MASK] [MASK] [MASK] [MASK] Ġmotivation, Ġjust Ġask Ġyourself Ġwhat Ġwould ĠJ es us Ġdo? ĠThe Ġanswer Ġwill Ġnever Ġbe Ġwrong. [SEP] [CLS] ĠWhen [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ĠCowboys Ġprogram, ĠI [MASK] [MASK] ately Ġint ri g ued [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġwork [MASK] [MASK] [MASK] all Ġship Ġwhile Ġvis [MASK] [MASK] Ġplaces [MASK] ĠAt Ġfirst, Ġit [MASK] [MASK] [MASK] [MASK] Ġunb elie v able [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġbe Ġtrue. ĠHowever, Ġafter Ġparticipating Ġmyself, ĠI Ġcan Ġconfident ly Ġsay Ġthat Ġthe Ġprogram Ġdel iver s Ġan Ġexperience Ġunlike [MASK] [MASK] [MASK] [MASK] Ġare Ġseveral Ġcomp elling Ġreasons Ġwhy ĠI Ġencourage Ġanyone Ġinterested Ġin Ġadventure [MASK] [MASK] [MASK] Ġjoin Ġus [MASK] ĠCowboys. Ġ Ċ Ċ One Ġof Ġthe Ġbest Ġparts Ġof Ġthe Ġprogram Ġis Ġall Ġof Ġthe Ġunique Ġloc ations Ġwe Ġget Ġto Ġvisit. ĠAs Ġmention ed Ġin Ġthe Ġarticle, Ġduring Ġmy Ġtime Ġwith ĠSeagoing ĠCowboys ĠI Ġtra veled [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] as k a, Ġsto pping Ġin\n================================================================================\ninput text:\n [MASK] Ġto Ġget Ġfrom ĠA m az on, [MASK] Ġask Ġour Ġparents Ġor [MASK] [MASK] ved Ġones [MASK] [MASK] [MASK] Ġtheir Ġopinion. ĠWhen Ġwe Ġdon [MASK] [MASK] [MASK] Ġcertain Ġsubject [MASK] [MASK], Ġwe Ġask Ġour Ġsiblings Ġfor Ġtheir Ġhelp, Ġso Ġthey [MASK] Ġteach [MASK] [MASK] [MASK] [MASK] [MASK] Ġopinions Ġfrom [MASK] [MASK] [MASK] Ġgreat Ġthing Ġto Ġdo Ġbecause Ġit Ġhelps Ġus Ġget Ġdifferent Ġperspectives, Ġand Ġcan Ġsave Ġus Ġin Ġschool. Ċ Ċ First Ġof Ġall, Ġpeople Ġask Ġothers Ġfor Ġtheir Ġopinions, Ġso Ġthey Ġcan Ġget Ġdifferent Ġview p oint s. ĠGetting Ġdifferent [MASK] [MASK] [MASK] s Ġhelp's Ġus Ġmake Ġour Ġdecision Ġfaster. ĠFor Ġexample, Ġif Ġwe Ġdon 't Ġknow [MASK] [MASK] e Ġto Ġget, Ġwe Ġwill Ġask Ġour Ġsiblings Ġor Ġparents Ġto Ġsee Ġwhat Ġthey [MASK] [MASK] [MASK] [MASK] Ġasking Ġour Ġlo ved Ġones, Ġwe Ġget Ġa [MASK] [MASK] [MASK] [MASK] Ġthis Ġcan Ġhelp Ġus [MASK] [MASK] Ġdecision Ġfaster. [MASK] [MASK] [MASK] [MASK] Ġhelp Ġus Ġmake Ġa Ġbetter [MASK] [MASK] [MASK] [MASK] [MASK], Ġgetting Ġmultiple Ġopinions Ġcan Ġsave Ġus Ġin Ġschool. ĠSome Ġpeople Ġmay Ġbe Ġstruggling Ġwith Ġa Ġcertain Ġsubject Ġin Ġschool [MASK] [MASK] [MASK] [MASK] [MASK] ĠGetting Ġhelp [MASK] [MASK] Ġin Ġthis Ġsituation Ġcan Ġhelp Ġus Ġpass Ġthe Ġclass Ġwith Ġan ĠA. ĠFor Ġexample, Ġwhen Ġwe [MASK] Ġstraight ĠA's, Ġwe Ġwill Ġask Ġour Ġsiblings, Ġwhich Ġalready Ġwent Ġthrough Ġmiddle Ġschool. ĠThey Ġwill Ġtell Ġus Ġa Ġfew Ġt ip s Ġand Ġtr [MASK] ks Ġwhich Ġmight Ġsave Ġus [MASK] [MASK] [MASK] ars hip Ġto Ġcollege. ĠAlthough Ġone Ġsib ling Ġmight [MASK] [MASK], Ġwe Ġstill Ġneed Ġto Ġask Ġothers Ġfor Ġtheir Ġopinions [MASK] [MASK] [MASK] [MASK] Ġus [MASK] [MASK] Ġcareer. ĠGetting Ġmultiple Ġopinions Ġcan Ġsave [MASK]. Ċ Ċ In Ġconclusion, Ġwe Ġshould [MASK] [MASK] Ġoutc Ġbecause Ġit Ġgives Ġus Ġdifferent Ġperspectives, Ġand Ġcan Ġsave Ġus Ġin Ġschool. ĠWhen Ġwe Ġget Ġdifferent Ġperspectives [MASK] [MASK] [MASK]'s Ġus Ġmake [MASK] [MASK] [MASK] [MASK] [MASK]. [MASK] [MASK] Ġsave Ġus Ġin Ġschool, [MASK] [MASK] [MASK] [MASK] [MASK] Ġand Ġgetting Ġstraight ĠA's [MASK] [MASK] [SEP] [CLS] ĠAlthough Ġsome Ġpeople Ġmay Ġfind [MASK] Ġonline Ġas [MASK] [MASK] [MASK] [MASK] Ġit Ġis Ġnot [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]. ĠIt Ġactually Ġcan Ġbe Ġreally Ġbeneficial Ġto Ġthe [MASK] [MASK] Ġfor Ġexample : Ġkids Ġwho Ġprefer Ġone Ġon Ġone Ġwith Ġthe Ġteacher, Ġkids Ġcan Ġfocus Ġbetter Ġwithout Ġhaving Ġtoo Ġmany Ġdistractions, Ġand Ġstudents Ġcan Ġgo Ġat Ġtheir Ġown Ġpace Ġwhen Ġlearning Ġthe Ġmaterial. ĠIn Ġthis Ġpaper, Ġwe Ġwill Ġbe Ġdiscus sing Ġhow Ġoffer ing Ġthe Ġchance Ġto Ġlearn Ġat Ġhome Ġcan Ġbe Ġgood Ġfor Ġthe Ġstudents. Ċ Ċ One Ġreason Ġwhy Ġkids Ġwould Ġbenefit Ġfrom Ġlearning Ġat Ġhome [MASK] Ġthey Ġwill Ġbe Ġable Ġto Ġfocus Ġmore. ĠL ots Ġof Ġkids Ġin Ġschool Ġseem Ġto Ġget Ġeasily Ġdistracted Ġby Ġfriends, Ġtheir Ġphone, Ġand Ġby Ġtheir Ġsurroundings Ġin Ġschool Ġwhether Ġit Ġbe Ġinside Ġor Ġoutside. ĠBut, Ġwith Ġlearning Ġonline Ġyou Ġcan Ġbe Ġin Ġthe Ġcomfort Ġof Ġyour Ġown Ġhome [MASK] [MASK] [MASK] Ġdistractions Ġas [MASK] [MASK] [MASK] Ġyou Ġattend Ġyour Ġclass Ġat Ġhome Ġyou 'll Ġbe Ġmore Ġfocused Ġand Ġget Ġmore [MASK] Ġdone. ĠWhich Ġwill Ġonly Ġhelp Ġyou Ġmore Ġin Ġthe Ġend. ĠWhen Ġit Ġcomes Ġto Ġbeing [MASK] [MASK], Ġthe Ġstudents Ġtend Ġto Ġget Ġeasily Ġdistracted Ġthan Ġthey Ġnormally Ġwould Ġbe Ġat Ġhome. [MASK] [MASK] [MASK] Ġchance Ġto Ġdo [MASK] [MASK] [MASK] [MASK] Ġonly Ġbenefit Ġthem Ġmore Ġfor Ġthe Ġfuture. ĠIf Ġthey [MASK] Ġable Ġto Ġfocus Ġmore Ġnow Ġat Ġhome Ġdoing [MASK] [MASK] Ġthat Ġamount Ġof Ġfocus Ġwill Ġst [MASK] [MASK] Ġthem Ġespecially Ġwhen Ġthey Ġhave Ġa Ġjob Ġor Ġwhen Ġthey're [MASK] [MASK] Ġcollege. Ċ Ċ The Ġsecond Ġreason [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġis Ġyou 'll Ġbe Ġable Ġto Ġgo Ġat Ġyour Ġown [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġgo Ġreally [MASK] [MASK] [MASK] [MASK] [MASK] Ġunderstand Ġa Ġsingle Ġthing Ġabout [MASK] [MASK] [MASK] ĠEven Ġwhen Ġtelling Ġa Ġteacher Ġto Ġslow Ġdown, Ġit Ġdoesn 't Ġalways Ġhelp. ĠThey Ġmight Ġspeed Ġright Ġback Ġup. ĠBut Ġwhen Ġit Ġcomes Ġto Ġlearning Ġat Ġhome, Ġyou're Ġable [MASK] Ġgo Ġas Ġslow Ġas Ġyou Ġwant Ġor Ġas Ġfast Ġas Ġyou Ġwant. ĠWhen Ġyou Ġlearn Ġat Ġyour Ġown Ġpace, Ġit Ġcan [MASK] [MASK] [MASK] [MASK] Ġsci [MASK] [MASK] [MASK] th Ġthan Ġyou Ġnormally Ġwould Ġin Ġschool. ĠG oing Ġat Ġyour Ġown [MASK] Ġcan [MASK] [MASK] [MASK] [MASK] Ġto Ġlearn Ġor Ġdo. ĠIt Ġalso Ġput [MASK] [MASK] Ġpressure Ġon Ġyou Ġtrying Ġto Ġlearn Ġthe Ġmaterial Ġon Ġthe Ġspot Ġat Ġthat Ġgiven Ġmoment. ĠLe arning Ġhow Ġto Ġwork Ġat Ġyour Ġown [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] Ġshow Ġyou Ġnot Ġto Ġrush Ġyour Ġwork Ġand Ġto Ġtake Ġtime Ġwith Ġdoing Ġit. Ċ [MASK] [MASK] Ġpeople Ġmight Ġhave Ġa Ġdifferent Ġopinion Ġwhen Ġit Ġcomes Ġto Ġonline Ġlearning [MASK] ĠSome Ġmay [MASK] [MASK] Ġwill Ġtake Ġadvantage Ġof Ġthis Ġopportunity Ġand Ġnot Ġdo Ġanything Ġor Ġthey Ġwill Ġsay Ġit [MASK] [MASK] [MASK] Ġas Ġeffective Ġas Ġphysically Ġbeing Ġin Ġschool Ġwith Ġothers. ĠOnline Ġlearning [MASK] [MASK] [MASK] Ġfor Ġeverybody Ġthough. ĠLike, Ġsome Ġkids Ġneed Ġa Ġteacher Ġto Ġbe [MASK] Ġwhen [MASK] [MASK] [MASK] [MASK] [MASK]'s Ġeasier Ġfor Ġthem Ġto Ġhave Ġsomeone Ġthere Ġat Ġall Ġtimes Ġand Ġnot Ġjust Ġthrough Ġa Ġscreen. ĠWith Ġthe Ġkids Ġwho Ġneed Ġteachers Ġto Ġbe Ġthere Ġfor Ġthem, Ġit Ġmight [MASK] Ġhard Ġto Ġadjust Ġto Ġthese Ġnew Ġacc omm od ations. ĠWhile Ġother [MASK] [MASK] [MASK] Ġalways Ġhave [MASK] [MASK] [MASK] [MASK] Ġinternet Ġor Ġpower. ĠWhich Ġwould Ġmean Ġsaid [MASK] [MASK] Ġnot Ġbe Ġable Ġto Ġattend Ġtheir Ġonline Ġclasses Ġand Ġget Ġany Ġwork Ġdone. Ċ Ċ In Ġconclusion, Ġonline Ġlearning Ġcan Ġbe Ġa [MASK] [MASK] [MASK] Ġthan Ġpeople Ġan t [MASK] [MASK] [MASK] [MASK] [MASK] Ġcan Ġbe Ġa Ġmore Ġeffective Ġtool Ġfor Ġkids Ġto Ġlearn Ġwhere Ġthey Ġare Ġmost Ġcom f y Ġwithout Ġall Ġthe Ġdistractions Ġaround Ġthem. ĠAlthough, Ġcertain Ġaspects Ġto Ġlearning Ġonline\n================================================================================\nTrain dataset size: 643\nTest dataset size: 3\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.34ba/s]\n100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.10ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.67ba/s]\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n# training updates per epoch: 20\n# training steps: 80\n# warmup steps: 4\nSTEP:    21/   80. LR: 35.2617. Loss: 7.6412. : : 21it [00:31,  1.50s/it]       \nSTEP:    42/   80. LR: 20.0000. Loss: 6.9830. : : 21it [00:31,  1.48s/it]       \nSTEP:    63/   80. LR: 4.7383. Loss: 6.7862. : : 21it [00:31,  1.50s/it]        \nSTEP:    84/   80. LR: 0.2728. Loss: 6.7336. : : 21it [00:31,  1.51s/it]        \nCPU times: user 2.11 s, sys: 415 ms, total: 2.53 s\nWall time: 2min 36s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 run_train_infer_deberta.py \\\n--config_path \"./configs/conf_deberta.yaml\" \\\n--save_dir \"./outputs\" \\\n--model_id \"m4\"","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:53:41.740200Z","iopub.execute_input":"2024-04-11T10:53:41.741043Z","iopub.status.idle":"2024-04-11T10:54:38.121392Z","shell.execute_reply.started":"2024-04-11T10:53:41.741003Z","shell.execute_reply":"2024-04-11T10:54:38.120328Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"2024-04-11 10:53:52.742616: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 10:53:52.742683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 10:53:52.744201: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-04-11 10:53:52.759239: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 10:53:52.759286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 10:53:52.760777: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n(137591, 4)\n(137591, 4)\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n           id     label\n0  e_0568mrf3  0.978877\n1  e_05d7vfch  0.963616\n2  e_0783dzbu  0.885889\n3  e_0autmw75  0.989657\n4  e_0dn8rra1  0.997957\nShape of PL: (150, 4)\nShape of train: (68069, 5)\nShape of train after merging PL: (68519, 3)\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\nTrain df shape: (1000, 3)\nValid df shape: (128, 5)\nTest df shape: (150, 3)\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.09ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.57ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.12ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.38ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.81ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.09ba/s]\n\n--- Sample 0 ---\n\n'[CLS] ĠDear ĠSt ate ĠSenator, Ċ Ċ I Ġam Ġwriting Ġto Ġyou Ġtoday Ġto Ġexpress Ġmy Ġsupport Ġfor Ġabolish ing Ġthe ĠElectoral ĠCollege Ġand Ġelect ing Ġthe ĠPresident Ġof Ġthe ĠUnited ĠStates Ġby Ġpopular Ġvote. ĠI Ġbelieve Ġthat Ġthis Ġchange Ġwould Ġbe Ġmore Ġdemocratic Ġand Ġwould Ġensure Ġthat Ġall ĠAmericans\\'Ġvo ices Ġare Ġheard Ġin Ġthe Ġelection Ġof Ġour Ġhigh est Ġoffice. Ċ Ċ The ĠElectoral ĠCollege Ġis Ġa Ġsystem Ġthat Ġwas Ġcreated Ġin Ġthe Ġ18 th Ġcent ury Ġto Ġgive Ġmore Ġpower Ġto Ġthe Ġsmall er Ġstates. ĠHowever, Ġit Ġis Ġno Ġlonger Ġnecessary Ġor Ġfair Ġin Ġthe Ġ2 1 st Ġcent ury. ĠIn Ġthe Ġlast Ġfew Ġelections, Ġthe Ġcandidate Ġwho Ġwon Ġthe Ġpopular Ġvote Ġhas Ġlost Ġthe Ġelection. ĠThis Ġis Ġun de m ocratic Ġand Ġmakes Ġit Ġseem Ġlike Ġsome Ġpeople\\'s Ġvotes Ġdon \\'t Ġcount. Ċ Ċ There Ġare Ġmany Ġarguments Ġin Ġfavor Ġof Ġabolish ing Ġthe ĠElectoral ĠCollege. ĠFirst, Ġit Ġis Ġun de m ocratic. ĠThe Ġcandidate Ġwho Ġwin s Ġthe Ġmost Ġvotes Ġshould Ġbe Ġthe ĠPresident. ĠSecond, Ġit Ġgives Ġtoo Ġmuch Ġpower Ġto Ġthe Ġsmall er Ġstates. ĠC ur rent ly, Ġa Ġcandidate Ġcan Ġwin Ġthe ĠPre s id ency Ġby Ġwinning Ġa Ġfew Ġsmall Ġstates Ġwith Ġa Ġlot Ġof Ġelectoral Ġvotes. ĠThis Ġmeans Ġthat Ġthe Ġmajority Ġof ĠAmericans Ġcan Ġbe Ġi g n ored. ĠTh ird, Ġit Ġis Ġunfair Ġto Ġthe Ġpeople Ġwho Ġlive Ġin Ġthe Ġstates Ġthat Ġare Ġalways Ġ\" sa fe \" Ġfor Ġone Ġparty. ĠIn Ġthese Ġstates, Ġthe Ġcandidates Ġdon \\'t Ġeven Ġb other Ġcampaign ing Ġbecause Ġthey Ġknow Ġthey Ġwill Ġwin Ġor Ġlose Ġregard less Ġof Ġthe Ġpopular Ġvote. ĠThis Ġmeans Ġthat Ġthe Ġpeople Ġin Ġthese Ġstates Ġhave Ġless Ġof Ġa Ġsay Ġin Ġthe Ġelection Ġthan Ġpeople Ġin Ġother Ġstates. Ċ Ċ There Ġare Ġa Ġfew Ġarguments Ġagainst Ġabolish ing Ġthe ĠElectoral ĠCollege. ĠFirst, Ġit Ġis Ġa Ġpart Ġof Ġour Ġhistory Ġand Ġtrad ition. ĠHowever, Ġtrad ition Ġis Ġnot Ġa Ġgood Ġreason Ġto Ġkeep Ġa Ġsystem Ġthat Ġis Ġun de m ocratic. ĠSecond, Ġit Ġcould Ġlead Ġto Ġa Ġtw op art y Ġsystem. ĠHowever, Ġthis Ġis Ġalready Ġthe Ġcase Ġin Ġthe ĠUnited ĠStates. ĠTh ird, Ġit Ġcould Ġlead Ġto Ġa ĠPresident Ġwho Ġdoes Ġnot Ġrepresent Ġthe Ġmajority Ġof ĠAmericans. ĠHowever, Ġthis Ġis Ġalready Ġpossible Ġwith Ġthe ĠElectoral ĠCollege. Ċ Ċ In Ġconclusion, ĠI Ġbelieve Ġthat Ġthe ĠElectoral ĠCollege Ġis Ġout d ated Ġand Ġun de m ocratic. ĠI Ġ ur ge Ġyou Ġto Ġsupport Ġa Ġconst itution al Ġam end ment Ġthat Ġwould Ġabolish Ġit Ġand Ġelect Ġthe ĠPresident Ġby Ġpopular Ġvote. Ċ Ċ Thank Ġyou Ġfor Ġyour Ġtime. [SEP]'\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n\n--- Sample 1 ---\n\n  0%|                                                     | 0/1 [00:00<?, ?ba/s]\"[CLS] ĠPhones Ġ& ĠDriving Ċ Ċ Many Ġpeople Ġnow ad ays Ġhave Ġa Ġphone Ġin Ġtheir Ġhands. ĠIt Ġis Ġalmost Ġimp o is ible Ġa o Ġget Ġaway Ġfrom Ġusing Ġone. ĠPeople Ġuse Ġthem Ġfor Ġso Ġmany Ġreasons Ġlike Ġtexting, Ġcall ing, Ġe ct. ĠBut Ġwhen Ġyou Ġare Ġbehind Ġthe Ġwheel Ġof Ġa Ġvehicle Ġit Ġshould Ġnot Ġbe Ġused Ġun le is Ġits Ġan Ġemergency. ĠThe Ġreason Ġbeing Ġbecause Ġthey Ġcan Ġcause Ġaccidents Ġor Ġeven Ġfatal ities. ĠI Ġbe a ieve Ġthat Ġdrivers Ġshouldn 't Ġdrive Ġwith Ġa Ġcell Ġphone Ġin Ġhand Ġbecause Ġit Ġtakes Ġyour Ġattention Ġoff Ġthe Ġroad. ĠWhen Ġsomeone Ġgets Ġon Ġh ish er Ġphone Ġwhile Ġdriving Ġh es he Ġput s Ġhim se af h es h ers elf Ġat Ġrisk Ġbut Ġalso Ġeveryone Ġelse Ġon Ġthe Ġstreet Ġas Ġwell. Ċ Ċ There fore, Ġphones Ġand Ġdriving Ġdo Ġnot Ġmix. ĠIf Ġyou Ġwere Ġgoing Ġto Ġsend Ġa Ġm eo s age Ġyou Ġshould Ġpull Ġover Ġsomewhere Ġsafe Ġuntil Ġit Ġis Ġdone Ġbec a u ie Ġif Ġyou Ġdon 't Ġyou Ġcould Ġbe Ġde ad Ġwithin Ġminutes. ĠFor Ġexample, Ġmy Ġmom Ġwas Ġdriving Ġto Ġwork Ġand Ġshe Ġhad Ġher Ġphone Ġin Ġher Ġl ap Ġand Ġshe Ġacc id en e ally Ġhit Ġthe Ġcall Ġbut t on Ġby Ġaccident Ġand Ġansw ered Ġthe Ġcall Ġwithout Ġreal iz ing Ġwhat Ġhappened. ĠShe Ġdidn 't Ġknow Ġthis Ġuntil Ġafter Ġo he Ġcra i h ed Ġinto Ġanother Ġcar Ġwhich Ġresult ed Ġin Ġan Ġinj ury Ġto Ġh ers elf Ġand Ġthe Ġother Ġdriver. ĠSo Ġshe Ġlearned Ġf ir s th and Ġwhy Ġwe Ġo h ould Ġnever Ġanswer Ġa Ġcall Ġwhile Ġdriving. ĠAnother Ġexample Ġwould Ġbe Ġif Ġsomeones Ġchildren Ġwere Ġto Ġplay Ġin Ġthe Ġstreet Ġand Ġa Ġperson Ġwas Ġtalking Ġto Ġsomeone Ġon Ġthe Ġphone Ġa he y Ġmight Ġnot Ġhear Ġthe Ġkid Ġrunning Ġout Ġinto Ġtraffic Ġwhich Ġcould Ġresult Ġin Ġan Ġaccident Ġleading Ġup Ġto Ġpossible Ġdeath. Ċ Ċ Some one Ġwho Ġdrives Ġwith Ġa Ġphone Ġin Ġhand Ġmay Ġthink Ġthat Ġthey Ġcan Ġhandle Ġdoing Ġboth Ġthings Ġat Ġonce Ġbut Ġthe Ġreality Ġis Ġthey Ġcannot. ĠM o it Ġhumans Ġonly Ġprocess Ġ3 Ġpie ces Ġinformation Ġat Ġany Ġgiven Ġtime Ġand Ġanything Ġbeyond Ġthree Ġpie ces Ġof Ġinform a i ion Ġwill Ġlead Ġto Ġconf usion Ġand Ġpossibly Ġbad Ġdecisions. ĠThat's Ġwhy Ġdrivers Ġneed Ġto Ġfocus Ġon Ġthe Ġroad Ġnot Ġa Ġscreen. ĠThey Ġneed Ġal a Ġtheir Ġsens es Ġin Ġorder Ġto Ġavoid Ġan Ġaccident. ĠA Ġhuman Ġbrain Ġneeds Ġconst ant Ġst im ul ation Ġto Ġkeep Ġi a Ġhealthy Ġtherefore Ġa Ġdriver Ġdoesn 't Ġneed Ġextra Ġdistractions Ġwhile Ġtrying Ġto Ġoper ate Ġa Ġmoving Ġobject Ġo u ch Ġas Ġa Ġcar. Ċ Ċ In Ġconcl u i ion, Ġno Ġone Ġshould Ġever Ġtalk Ġor Ġtext Ġwhile Ġoperating Ġa Ġmotor ized Ġvehicle. ĠThere Ġare Ġtoo Ġmany Ġconsequences Ġfor Ġsomething Ġthat Ġi im ple. ĠAs Ġlong Ġas Ġwe Ġcontinue Ġto Ġlet Ġthese Ġhab it o [SEP]\"\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n\n--- Sample 2 ---\n\n\"[CLS] ĠSome Ġschools Ġoffer Ġdistance Ġlearning Ġas Ġan Ġoption Ġfor Ġstudents Ġto Ġattend Ġclasses Ġfrom Ġhome Ġby Ġway Ġof Ġonline Ġor Ġvideo Ġconferencing. ĠThis Ġis Ġa Ġbad Ġidea Ġbecause Ġthere Ġare Ġso Ġmany Ġproblems Ġwith Ġit Ġthat Ġcould Ġcause Ġstudents Ġnot Ġto Ġget Ġthe Ġfull Ġexperience Ġand Ġeducation Ġthey Ġneed Ġat Ġschool. ĠSome Ġreasons Ġwhy Ġthis Ġwould Ġbe Ġa Ġvery Ġbad Ġidea, Ġis Ġthat Ġsome Ġkids Ġmight Ġhave Ġtrouble Ġunderstanding Ġwhat Ġthey're Ġbeing Ġtaught Ġin Ġclass Ġif Ġthey Ġwere Ġto Ġtry Ġlearn Ġit Ġthrough Ġvideo Ġch at. ĠAnother Ġreason Ġwhy Ġattending Ġclasses Ġfrom Ġhome Ġwouldn 't Ġwork Ġout, Ġis Ġthat Ġkids Ġneed Ġsocial Ġinteract ion Ġand Ġtime Ġwith Ġfriends Ġto Ġhelp Ġthem Ġgrow Ġup Ġinto Ġgood Ġadults, Ġbut Ġwithout Ġa Ġsocial Ġlife Ġlike Ġgoing Ġto Ġschool Ġeveryday Ġthey Ġwon 't Ġknow Ġhow Ġto Ġinteract Ġwith Ġother Ġpeople. ĠLastly, Ġwhen Ġyou Ġgo Ġto Ġschool Ġyou Ġget Ġthe Ġreal Ġexperience Ġon Ġthings Ġthat Ġaren 't Ġjust Ġshown Ġto Ġyou Ġon Ġa Ġscreen. ĠIf Ġall Ġwe Ġdo Ġis Ġsit Ġaround Ġwatching Ġsc re ens Ġall Ġday Ġthen Ġwho Ġwill Ġbuild Ġthings Ġlike Ġbri d ges Ġand Ġbuild ings? ĠWe Ġwont Ġever Ġhave Ġanyone Ġdoing Ġthose Ġjobs Ġanymore. ĠIt's Ġimportant Ġthat Ġeveryone Ġgets Ġa Ġwell Ġ round ed edu c ation, Ġwhich Ġisn 't Ġpossible Ġif Ġyour Ġstu ck Ġat Ġhome Ġst aring Ġat Ġa Ġcomputer Ġscreen Ġall Ġday. ĠThere Ġare Ġa Ġlot Ġmore Ġposit ives Ġabout Ġgoing Ġto Ġpublic Ġschool Ġthan Ġstaying Ġhome Ġand Ġhaving Ġsomeone Ġteach Ġyou Ġthrough Ġa Ġscreen. ĠAt t ending Ġclasses Ġfrom Ġhome Ġis Ġdefinitely Ġnot Ġthe Ġright Ġchoice Ġbecause Ġthere Ġare Ġso Ġmany Ġnegative Ġeffects Ġto Ġit Ġcompared Ġto Ġactually Ġcoming Ġto Ġschool Ġevery Ġmorning. [SEP]\"\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n\n--- Sample 3 ---\n\n'[CLS] ĠThe Ġadvantages Ġthat Ġnot Ġhaving Ġcars Ġcan Ġbring Ġare Ġmany, Ġsome Ġmay Ġlook Ġvery Ġfar Ġout Ġof Ġreality Ġbut Ġthis Ġis Ġhappening Ġin Ġa Ġworld Ġin Ġwhich Ġmost Ġpeople Ġjust Ġtake Ġtheir Ġcars Ġwhen Ġthey Ġgo Ġany Ġwhere Ġ, Ġso Ġif Ġwe Ġhave Ġcar Ġfree Ġdays Ġthat Ġwe Ġgo Ġout Ġand Ġride Ġour Ġbi kes Ġor Ġwalking Ġcan Ġactually Ġbring Ġsome Ġpositive Ġchanges Ġthat Ġmight Ġtake Ġplace Ġ. In Ġsome Ġcountries Ġlike ĠVauban, Ġg erm any Ġwhere Ġthey Ġsay Ġ\" Ġwhen Ġwe Ġhad Ġa Ġcar Ġi Ġalways Ġseem ed Ġtense Ġ, Ġand Ġnow Ġi\\'m Ġmuch Ġmore Ġhappier Ġnot Ġdriving Ġanymore Ġ\" Ġthey Ġsay Ġthat Ġbecause Ġin Ġthose Ġdays Ġall Ġcars Ġwere Ġbanned Ġfrom Ġthat Ġparticular Ġarea. ĠThis Ġcity Ġis Ġa Ġsub urb an Ġand Ġis Ġ8 0 Ġpercent Ġwith Ġout Ġcars Ġand Ġ2 9 Ġper ce m t Ġof Ġpeople Ġhave Ġa Ġcar Ġ, Ġthis Ġplace Ġis Ġgoing Ġto Ġcut Ġdown Ġmore Ġcars Ġthat Ġare Ġon Ġthe Ġroad Ġthat Ġcould Ġsave Ġfuel Ġand Ġmake Ġa Ġplace Ġcleaner Ġof Ġall Ġthe Ġpolution Ġthat Ġis Ġgetting Ġinto Ġthe Ġpeople\\'s Ġl un gs Ġwhich Ġwill Ġmake Ġmore Ġsick Ġpeople Ġin Ġthat Ġworld Ġ. ĠThe Ġadvantage Ġof Ġthis Ġcan Ġhelp Ġprevent Ġtraffic Ġj ams Ġall Ġover Ġand Ġthe Ġhigh Ġpric es Ġthat Ġpeople Ġpay Ġfor Ġins ur aces Ġto Ġput Ġon Ġtheir Ġvehicles Ġ. The Ġcar Ġfree Ġareas Ġhave Ġbecome Ġone Ġof Ġmost Ġrecent Ġin ĠBog ot, ĠColombia Ġ. Ġthe Ġcar Ġfree Ġzone Ġhas Ġbecome Ġa Ġgood Ġway Ġto Ġget Ġa Ġgood Ġwork out Ġand Ġreduce Ġt ens ions Ġfrom Ġevery Ġone Ġ, Ġpeople Ġcan Ġnot Ġonly Ġenjoy Ġbut Ġhave Ġfun Ġplaying Ġsports Ġor Ġjust Ġsit Ġthere Ġto Ġrelax Ġand Ġsee Ġthe Ġsc ener y Ġfrom Ġover Ġthere Ġ. Ġ1998 Ġcar Ġfree Ġday Ġbe gan Ġin Ġb og ata Ġand Ġ199 9 Ġwas Ġwhen Ġthis Ġwas Ġcalled Ġ\\' day Ġwithout Ġcars\\'Ġ. Ġthis Ġday Ġhas Ġmade Ġevery Ġthing Ġthat Ġyou Ġthink Ġis Ġnot Ġimportant Ġseem Ġless Ġimportant. ĠPeople Ġhave Ġa Ġsense Ġof Ġbeing Ġtogether Ġand Ġmake Ġthem Ġfeel Ġgood Ġabout Ġthemselves Ġbecause Ġthey Ġdont Ġfeel Ġso Ġstressed Ġout Ġanymore Ġ. Ċ Ċ \" In Ġthe Ġfuture Ġcities Ġwont \\'t Ġlook Ġthis Ġcar int ense Ġ. Ġwe Ġhave Ġto Ġthink Ġout Ġthese Ġnew Ġinnov ations Ġand Ġnew Ġthings Ġand Ġwe Ġcould Ġlive Ġin Ġhealth ier Ġlife Ġand Ġnot Ġonly Ġget Ġrid Ġof Ġthat Ġpolution Ġbut Ġget Ġrid Ġof Ġit Ġall Ġtogether Ġ\" Ġ ĠE l is ab e th ĠR os ent h al Ġof Ġthe ĠN Y T Ġ. ĠThis Ġhas Ġbeen Ġone Ġbig Ġissue Ġwhen Ġit Ġcomes Ġto Ġnot Ġhave Ġmuch Ġpol t u ion Ġgoing Ġthrough Ġour Ġstreets Ġbut Ġjust Ġa Ġlot Ġof Ġgood Ġand Ġhelpful Ġthings Ġin Ġthese Ġthings, Ġand Ġthis Ġwill Ġhelp Ġprevent Ġany Ġharm Ġon Ġother Ġlives Ġthat Ġare Ġaround Ġus Ġ. This Ġis Ġone Ġtime Ġin Ġmy Ġlife Ġwhen Ġall Ġmy Ġdre ams Ġare Ġabout Ġcars Ġand Ġthere Ġabout Ġthe Ġenvir an em et Ġwith Ġcars Ġ. ĠIn Ġmy Ġcity Ġthat Ġwas Ġa Ġlot Ġof Ġcars Ġwith Ġout Ġthese [SEP]'\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ./models/deberta_v3_small_persuade and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.59s/ba]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.57ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  7.25ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.60ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.49ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.72ba/s]\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ./models/deberta_v3_small_persuade and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n  0%|                                                    | 0/62 [00:00<?, ?it/s][W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n{'loss': 0.684, 'grad_norm': 0.8839093446731567, 'learning_rate': 9.685230024213076e-06, 'epoch': 0.8}\n{'train_runtime': 18.4927, 'train_samples_per_second': 54.075, 'train_steps_per_second': 3.353, 'train_loss': 0.6820432601436492, 'epoch': 0.99}\n100%|███████████████████████████████████████████| 62/62 [00:18<00:00,  3.36it/s]\n100%|███████████████████████████████████████████| 19/19 [00:02<00:00,  9.13it/s]\ndone!\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nCPU times: user 789 ms, sys: 190 ms, total: 979 ms\nWall time: 56.4 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile run_ahmet_approach.py\n\nimport argparse\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sparse_dot_topn import awesome_cossim_topn\n\n\nclass TextMatcher:\n    def __init__(self, ground_truth, col, topk=5, lower_bound=-1):\n        self.ground_truth = ground_truth\n        self.vec = TfidfVectorizer(ngram_range=(1, 2), analyzer=\"word\", token_pattern=r\"(?u)(\\b\\w\\w+\\b|[\\.,!])\",\n                                   use_idf=False, min_df=2, binary=True)\n        self.topk = topk\n        self.lower_bound = lower_bound\n        self.col = col\n        \n    def get_matches_df(self, sparse_matrix, texts):\n        non_zeros = sparse_matrix.nonzero()\n\n        text_indices = non_zeros[0]\n        gt_indices = non_zeros[1]\n\n        left_side = np.empty(gt_indices.size, dtype=object)\n        right_side = np.empty(gt_indices.size, dtype=object)\n        match_score = np.zeros(gt_indices.size)\n\n        for index in range(gt_indices.size):\n            left_side[index] = texts.values[text_indices[index]]\n            right_side[index] = self.ground_truth[self.col].values[gt_indices[index]]\n            match_score[index] = sparse_matrix.data[index]\n\n        res_df = pd.DataFrame({self.col: left_side,\n                               'Ground Truth': right_side,\n                               'match_score': match_score})\n\n        res_df = pd.DataFrame(texts).merge(res_df, on=self.col, how=\"left\")\n        return res_df\n\n\n    def match(self, texts_to_match, n_threads=16):\n        print(f\"Matching {texts_to_match.shape[0]} texts to {self.ground_truth.shape[0]} texts...\")\n        \n        X = self.vec.fit_transform(texts_to_match[self.col])\n        X_gt = self.vec.transform(self.ground_truth[self.col])\n        \n        sparse_sim = awesome_cossim_topn(X, X_gt.T, self.topk, self.lower_bound, use_threads=True, n_jobs=n_threads)\n        \n        return self.get_matches_df(sparse_sim, texts_to_match[self.col])\n\n#---------------------------------------------------------------------------------------------------------#\n# DATA\n#---------------------------------------------------------------------------------------------------------#\n\nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument('--score_path', type=str, required=True)\n    ap.add_argument('--save_dir', type=str, required=True)\n    ap.add_argument('--model_id', type=str, required=True)\n\n    args = ap.parse_args()\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        df = pd.read_csv(\"/kaggle/input/textdata/test_essays.csv\", sep=',')\n    else:\n        df = pd.read_csv(\"/kaggle/input/textdata/test.csv\", sep=',')\n\n    all_prompts = df[\"prompt_id\"].unique()\n\n    HUMAN_TH = 0.1\n    LLM_TH = 0.99\n    MIN_N = min(64, len(df))\n\n    # Weak Supervision ---\n    scores_df = pd.read_parquet(args.score_path)\n    df = pd.merge(df, scores_df, on='id', how='inner')\n\n    df_list = []\n    for pid in all_prompts:\n        cdf = df[df['prompt_id']==pid].copy() \n        cdf = cdf.sort_values(by='generated').reset_index(drop=True)\n\n        cdf[\"likely_student\"] = cdf[\"generated\"].apply(lambda x: x<=HUMAN_TH)\n        cdf[\"likely_llm\"] = cdf[\"generated\"].apply(lambda x: x>=LLM_TH)\n\n        if cdf[\"likely_student\"].sum() < MIN_N:\n\n            cdf.loc[:MIN_N, \"likely_student\"] = True\n\n        if cdf[\"likely_llm\"].sum() < MIN_N:\n            cdf.loc[cdf.shape[0] - MIN_N:, \"likely_llm\"] = True\n        print(cdf.head())\n        print(cdf.tail())\n        print(\"==\"*40)\n        df_list.append(cdf)\n\n    df = pd.concat(df_list).reset_index(drop=True)\n    df = df.drop(columns=['generated'])\n\n    #---------------------------------------------------------------------------------------------------------#\n    # MATCHING\n    #---------------------------------------------------------------------------------------------------------#\n\n    TOPK = min(64, len(df))\n    \n    def agg_fn(scores, margin=0.5):\n        max_score = max(scores)\n        th = (1.0 - margin) * max_score\n        kept_scores = [s for s in scores if s >= th]\n        ret = np.mean(kept_scores)\n        return ret\n\n    def get_match_score(df, gt_filter_col):\n        tm = TextMatcher(df[df[gt_filter_col]].reset_index(drop=True), \"text\", topk=TOPK)\n        res_df = tm.match(df, n_threads=4)\n        df = res_df.groupby(\"text\")[\"match_score\"].agg(agg_fn).reset_index().merge(df, on=\"text\")\n        return df\n\n\n    sub_dfs = [get_match_score(df[df[\"prompt_id\"] == pid], \"likely_student\").reset_index(drop=True)[[\"id\", \"match_score\"]]\n               for pid in all_prompts] # TODO: may cause exception?\n    sub_df = pd.concat(sub_dfs).rename(columns={\"match_score\": \"match_score_student\"})\n\n\n    sub_dfs = [get_match_score(df[df[\"prompt_id\"] == pid], \"likely_llm\").reset_index(drop=True)[[\"id\", \"match_score\"]]\n               for pid in all_prompts]\n    sub_df2 = pd.concat(sub_dfs).rename(columns={\"match_score\": \"match_score_llm\"})\n\n    sub_df = sub_df.merge(sub_df2, on=\"id\")\n\n    SMOOTH = 0.15\n\n    sub_df[\"generated\"] = -sub_df[\"match_score_student\"] / (sub_df[\"match_score_llm\"] + SMOOTH)\n    sub_df = sub_df[[\"id\", \"generated\"]].copy()\n\n    sub_df.to_parquet(f\"./{args.save_dir}/{args.model_id}.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:55:08.159298Z","iopub.execute_input":"2024-04-11T10:55:08.159694Z","iopub.status.idle":"2024-04-11T10:55:08.169182Z","shell.execute_reply.started":"2024-04-11T10:55:08.159660Z","shell.execute_reply":"2024-04-11T10:55:08.168182Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Writing run_ahmet_approach.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile run_ahmet_approach_with_train_leverage.py\n\nimport argparse\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sparse_dot_topn import awesome_cossim_topn\nfrom copy import deepcopy\n\nclass TextMatcher:\n    def __init__(self, ground_truth, col, topk=5, lower_bound=-1):\n        self.ground_truth = ground_truth\n        self.vec = TfidfVectorizer(ngram_range=(1, 2), analyzer=\"word\", token_pattern=r\"(?u)(\\b\\w\\w+\\b|[\\.,!])\",\n                                   use_idf=False, min_df=2, binary=True)\n        self.topk = topk\n        self.lower_bound = lower_bound\n        self.col = col\n        \n    def get_matches_df(self, sparse_matrix, texts):\n        non_zeros = sparse_matrix.nonzero()\n\n        text_indices = non_zeros[0]\n        gt_indices = non_zeros[1]\n\n        left_side = np.empty(gt_indices.size, dtype=object)\n        right_side = np.empty(gt_indices.size, dtype=object)\n        match_score = np.zeros(gt_indices.size)\n\n        for index in range(gt_indices.size):\n            left_side[index] = texts.values[text_indices[index]]\n            right_side[index] = self.ground_truth[self.col].values[gt_indices[index]]\n            match_score[index] = sparse_matrix.data[index]\n\n        res_df = pd.DataFrame({self.col: left_side,\n                               'Ground Truth': right_side,\n                               'match_score': match_score})\n\n        res_df = pd.DataFrame(texts).merge(res_df, on=self.col, how=\"left\")\n        return res_df\n\n\n    def match(self, texts_to_match, n_threads=16):\n        print(f\"Matching {texts_to_match.shape[0]} texts to {self.ground_truth.shape[0]} texts...\")\n        \n        X = self.vec.fit_transform(texts_to_match[self.col])\n        X_gt = self.vec.transform(self.ground_truth[self.col])\n        \n        sparse_sim = awesome_cossim_topn(X, X_gt.T, self.topk, self.lower_bound, use_threads=True, n_jobs=n_threads)\n        \n        return self.get_matches_df(sparse_sim, texts_to_match[self.col])\n\n#---------------------------------------------------------------------------------------------------------#\n# DATA\n#---------------------------------------------------------------------------------------------------------#\n\nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument('--save_dir', type=str, required=True)\n    ap.add_argument('--model_id', type=str, required=True)\n\n    args = ap.parse_args()\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        test_df = pd.read_csv(\"/kaggle/input/textdata/test_essays.csv\", sep=',')\n    else:\n        test_df = pd.read_csv(\"/kaggle/input/textdata/test.csv\", sep=',')\n        \n    # add in prompt name ---\n    gdf = test_df.groupby(\"prompt_id\")[\"id\"].agg(len).reset_index().rename(columns={\"id\": \"count\"})\n    gdf = gdf.sort_values(by='count')\n\n    prompt_order = [\n        'Facial action coding system',\n        'Exploring Venus',\n        'A Cowboy Who Rode the Waves',\n        'The Face on Mars',\n        'Driverless cars',\n    ]\n\n    gdf['prompt_name'] = prompt_order[:len(gdf)]\n    prompt_id2prompt_name = dict(zip(gdf['prompt_id'], gdf['prompt_name']))\n    test_df['prompt_name'] = test_df['prompt_id'].map(prompt_id2prompt_name)\n    print(test_df.head())\n    \n    #-----------------------\n\n    all_prompt_names = test_df[\"prompt_name\"].unique().tolist()\n    \n    # prepare lables --\n    train_df = pd.read_parquet(\"/kaggle/input/d402-prepare-train-for-retrieval/train_for_retrieval.parquet\")\n    train_df = train_df.drop_duplicates(subset=['text']).reset_index(drop=True)\n    \n    train_df['likely_student'] = train_df['generated'].apply(lambda x: x<=0.001)\n    train_df['likely_llm'] = train_df['generated'].apply(lambda x: x>=0.999)\n\n    train_df = train_df.drop(columns=['generated'])\n\n    #---------------------------------------------------------------------------------------------------------#\n    # MATCHING\n    #---------------------------------------------------------------------------------------------------------#\n\n    \n    TOPK = min(64, len(test_df))\n    \n    def agg_fn(scores, margin=0.5):\n        max_score = max(scores)\n        th = (1.0 - margin) * max_score\n        kept_scores = [s for s in scores if s >= th]\n        ret = np.mean(kept_scores)\n        return ret\n\n    def get_match_score(true_df, infer_df):\n        true_df = deepcopy(true_df)\n        infer_df = deepcopy(infer_df)\n        \n        true_df = true_df.reset_index(drop=True)\n        tm = TextMatcher(true_df, \"text\", topk=TOPK)\n        \n        res_df = tm.match(infer_df, n_threads=4)\n        df = res_df.groupby(\"text\")[\"match_score\"].agg(agg_fn).reset_index().merge(infer_df, on=\"text\")\n        return df\n\n\n    # human match --\n    sub_dfs = []\n    for pname in prompt_order:\n        pdf = train_df[train_df['prompt_name'] == pname].copy()\n        true_df = pdf[pdf['likely_student']].copy()\n        \n        infer_df = test_df[test_df['prompt_name'] == pname].copy()\n        infer_df = infer_df.reset_index(drop=True)\n        \n        r = get_match_score(true_df, infer_df)\n        sub_dfs.append(r)\n        \n    sub_df_human = pd.concat(sub_dfs).rename(columns={\"match_score\": \"match_score_student\"})\n    \n    # LLM match --\n    sub_dfs = []\n    for pname in prompt_order:\n        pdf = train_df[train_df['prompt_name'] == pname].copy()\n        true_df = pdf[pdf['likely_llm']].copy()\n        \n        infer_df = test_df[test_df['prompt_name'] == pname].copy()\n        infer_df = infer_df.reset_index(drop=True)\n        \n        r = get_match_score(true_df, infer_df)\n        sub_dfs.append(r)\n        \n    sub_df_llm = pd.concat(sub_dfs).rename(columns={\"match_score\": \"match_score_llm\"})\n    #----\n\n    sub_df = pd.merge(sub_df_human, sub_df_llm, on=\"id\")\n    print(sub_df.head())\n\n    SMOOTH = 0.15\n\n    sub_df[\"generated\"] = -sub_df[\"match_score_student\"] / (sub_df[\"match_score_llm\"] + SMOOTH)\n    sub_df = sub_df[[\"id\", \"generated\"]].copy()\n\n    sub_df.to_parquet(f\"./{args.save_dir}/{args.model_id}.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:55:12.010126Z","iopub.execute_input":"2024-04-11T10:55:12.010498Z","iopub.status.idle":"2024-04-11T10:55:12.020238Z","shell.execute_reply.started":"2024-04-11T10:55:12.010462Z","shell.execute_reply":"2024-04-11T10:55:12.019254Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Writing run_ahmet_approach_with_train_leverage.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python run_ahmet_approach.py \\\n--score_path ./outputs/m0.parquet \\\n--save_dir \"./outputs\" \\\n--model_id \"m5\"","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:55:16.585620Z","iopub.execute_input":"2024-04-11T10:55:16.586325Z","iopub.status.idle":"2024-04-11T10:55:19.787899Z","shell.execute_reply.started":"2024-04-11T10:55:16.586272Z","shell.execute_reply":"2024-04-11T10:55:19.786791Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"           id  prompt_id  ... likely_student  likely_llm\n0  e_lvrfnxan          5  ...           True        True\n1  e_9lzxlyp0          5  ...           True        True\n2  e_knxffsz4          5  ...           True        True\n3  e_1ituvpmv          5  ...           True        True\n4  e_cv19bwix          5  ...           True        True\n\n[5 rows x 6 columns]\n           id  prompt_id  ... likely_student  likely_llm\n5  e_yshmqlkt          5  ...           True        True\n6  e_g0adeocf          5  ...           True        True\n7  e_c08bnr4e          5  ...           True        True\n8  e_4kog18mm          5  ...           True        True\n9  e_u2pku451          5  ...           True        True\n\n[5 rows x 6 columns]\n================================================================================\n           id  prompt_id  ... likely_student  likely_llm\n0  e_sbwqyx5l          6  ...           True        True\n1  e_371zu85b          6  ...           True        True\n2  e_oy0mocld          6  ...           True        True\n3  e_y8cupfs6          6  ...           True        True\n4  e_xltwlys8          6  ...           True        True\n\n[5 rows x 6 columns]\n            id  prompt_id  ... likely_student  likely_llm\n15  e_cyq2lz6b          6  ...           True        True\n16  e_97e3nhvw          6  ...           True        True\n17  e_k78aw69y          6  ...           True        True\n18  e_dici2904          6  ...           True        True\n19  e_swg6uvuq          6  ...           True        True\n\n[5 rows x 6 columns]\n================================================================================\n           id  prompt_id  ... likely_student  likely_llm\n0  e_1r0460zu          3  ...           True        True\n1  e_dauhezfh          3  ...           True        True\n2  e_mt4txq47          3  ...           True        True\n3  e_o49u4x98          3  ...           True        True\n4  e_23u2i00m          3  ...           True        True\n\n[5 rows x 6 columns]\n            id  prompt_id  ... likely_student  likely_llm\n25  e_s2m3mzdn          3  ...           True        True\n26  e_r9l7fsbf          3  ...           True        True\n27  e_w2gukx33          3  ...           True        True\n28  e_868wnojl          3  ...           True        True\n29  e_l3c5gy6c          3  ...           True        True\n\n[5 rows x 6 columns]\n================================================================================\n           id  prompt_id  ... likely_student  likely_llm\n0  e_n73jxwrq          2  ...           True        True\n1  e_awg6lcfd          2  ...           True        True\n2  e_bj11b3jo          2  ...           True        True\n3  e_cc82fgh3          2  ...           True        True\n4  e_h2thjzts          2  ...           True        True\n\n[5 rows x 6 columns]\n            id  prompt_id  ... likely_student  likely_llm\n35  e_zwnpnql2          2  ...           True        True\n36  e_mrcm6uop          2  ...           True        True\n37  e_30kag848          2  ...           True        True\n38  e_walgqxs7          2  ...           True        True\n39  e_f5z1xe6h          2  ...           True        True\n\n[5 rows x 6 columns]\n================================================================================\n           id  prompt_id  ... likely_student  likely_llm\n0  e_bv3sjl0f          4  ...           True        True\n1  e_m1ax6w1u          4  ...           True        True\n2  e_52k418eb          4  ...           True        True\n3  e_4uy3khtg          4  ...           True        True\n4  e_e3iyuazx          4  ...           True        True\n\n[5 rows x 6 columns]\n            id  prompt_id  ... likely_student  likely_llm\n45  e_x5coxi3a          4  ...           True        True\n46  e_a7mg6vfb          4  ...           True        True\n47  e_bb2nkygy          4  ...           True        True\n48  e_isqnipm4          4  ...           True        True\n49  e_ysgbbvip          4  ...           True        True\n\n[5 rows x 6 columns]\n================================================================================\nMatching 10 texts to 10 texts...\nMatching 20 texts to 20 texts...\nMatching 30 texts to 30 texts...\nMatching 40 texts to 40 texts...\nMatching 50 texts to 50 texts...\nMatching 10 texts to 10 texts...\nMatching 20 texts to 20 texts...\nMatching 30 texts to 30 texts...\nMatching 40 texts to 40 texts...\nMatching 50 texts to 50 texts...\n","output_type":"stream"}]},{"cell_type":"code","source":"!python run_ahmet_approach_with_train_leverage.py \\\n--save_dir \"./outputs\" \\\n--model_id \"m6\"","metadata":{"execution":{"iopub.status.busy":"2024-04-11T10:55:28.638774Z","iopub.execute_input":"2024-04-11T10:55:28.639753Z","iopub.status.idle":"2024-04-11T10:56:18.642264Z","shell.execute_reply.started":"2024-04-11T10:55:28.639716Z","shell.execute_reply":"2024-04-11T10:56:18.641154Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"           id  ...                  prompt_name\n0  e_4kog18mm  ...  Facial action coding system\n1  e_9lzxlyp0  ...  Facial action coding system\n2  e_c08bnr4e  ...  Facial action coding system\n3  e_lvrfnxan  ...  Facial action coding system\n4  e_1ituvpmv  ...  Facial action coding system\n\n[5 rows x 4 columns]\nMatching 10 texts to 2656 texts...\nMatching 20 texts to 2322 texts...\nMatching 30 texts to 1972 texts...\nMatching 40 texts to 2132 texts...\nMatching 50 texts to 2309 texts...\nMatching 10 texts to 10621 texts...\nMatching 20 texts to 10619 texts...\nMatching 30 texts to 9112 texts...\nMatching 40 texts to 9847 texts...\nMatching 50 texts to 10491 texts...\n                                              text_x  ...                prompt_name_y\n0  #Technology Has Changed Over Past Decade Essay...  ...  Facial action coding system\n1  5 reasons the technology could be valuable are...  ...  Facial action coding system\n2  Computers don t need to know if someone is fee...  ...  Facial action coding system\n3  I believe that using technology to read the em...  ...  Facial action coding system\n4  I really belief that this new techology of Fac...  ...  Facial action coding system\n\n[5 rows x 9 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile run_deb_ranking_inference_rb.py\n\nimport sys\n\nsys.path.insert(0, '/kaggle/input/omegaconf')\nsys.path.insert(0, '/kaggle/input/utils-ai-v10')\n\nimport argparse\nimport os\nimport gc\n\nimport pandas as pd\nimport torch\nfrom accelerate import Accelerator\nfrom omegaconf import OmegaConf\nfrom peft import PeftModel\n\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n\nfrom r_ranking.ai_dataset import AiDataset\nfrom r_ranking.ai_loader import AiCollator, show_batch\nfrom r_ranking.ai_model import AiModel\n\nchar_to_remove = ['{', '£', '\\x97', '¹', 'å', '\\\\', '\\x85', '<', '\\x99', \\\n                  'é', ']', '+', 'Ö', '\\xa0', '>', '|', '\\x80', '~', '©', \\\n                  '/', '\\x93', '$', 'Ó', '²', '^', ';', '`', 'á', '*', '(', \\\n                  '¶', '®', '[', '\\x94', '\\x91', '#', '-', 'ó', ')', '}', '=']\n\ndef preprocess_text(text):\n    text = text.encode(\"ascii\", \"ignore\").decode('ascii')        \n    text = text.strip()\n    text = text.strip(\"\\\"\")\n\n    for c in char_to_remove:\n        text = text.replace(c, \"\")\n\n    if text[-1]!=\".\":\n        text = text.split(\".\")\n        text = \".\".join(text[:-1])\n        text += \".\"\n    return text\n\n\ndef run_inference(accelerator, model, infer_dl, example_ids):\n    model.eval()\n    all_predictions = []\n\n    progress_bar = tqdm(range(len(infer_dl)), disable=not accelerator.is_local_main_process)\n\n    for step, batch in enumerate(infer_dl):\n        with torch.no_grad():\n            logits, _ = model(**batch)\n\n        logits = logits.reshape(-1)\n        predictions = torch.sigmoid(logits)\n        predictions = accelerator.gather_for_metrics(predictions)\n        predictions = predictions.cpu().numpy().tolist()\n\n        all_predictions.extend(predictions)\n\n        progress_bar.update(1)\n    progress_bar.close()\n\n    result_df = pd.DataFrame()\n    result_df[\"id\"] = example_ids\n    result_df[\"generated\"] = all_predictions\n\n    return result_df\n\ndef main(cfg, save_dir, model_id):\n    \n    # create accelerator\n    accelerator = Accelerator()\n    \n    # read test data\n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        test_df = pd.read_csv(\"/kaggle/input/textdata/test_essays.csv\", sep=',')\n    else:\n        test_df = pd.read_csv(\"/kaggle/input/textdata/test.csv\", sep=',')\n\n    test_df['text'] = test_df['text'].apply(preprocess_text)\n    accelerator.print(f'Test csv shape: {test_df.shape}')\n    test_df['generated'] = 1 # TODO: NEEDED NOW, FIx it \n    \n    with accelerator.main_process_first():\n        dataset_creator = AiDataset(cfg)\n        infer_ds = dataset_creator.get_dataset(test_df)\n    \n    tokenizer = dataset_creator.tokenizer\n    \n    infer_ds = infer_ds.sort(\"input_length\")\n    infer_ds.set_format(\n        type=None,\n        columns=[\n            'id',\n            'input_ids',\n            'attention_mask',\n            'generated'\n        ]\n    )\n    \n    infer_ids = infer_ds[\"id\"]  # .tolist()\n    \n    #--\n    data_collator = AiCollator(\n        tokenizer=tokenizer,\n        pad_to_multiple_of=64\n    )\n\n    infer_dl = DataLoader(\n        infer_ds,\n        batch_size=cfg.predict_params.per_device_eval_batch_size,\n        shuffle=False,\n        collate_fn=data_collator,\n    )\n\n    accelerator.print(\"data preparation done...\")\n    accelerator.print(\"~~\"*40)\n    accelerator.wait_for_everyone()\n    \n    \n    #----------\n    for b in infer_dl:\n        break\n    show_batch(b, tokenizer, task='infer', print_fn=accelerator.print)\n    accelerator.print(\"~~\"*40)\n    #----------\n    # model -----------------------------------------------------------------------------#\n    model = AiModel(cfg, accelerator.device)\n\n    checkpoint_path = cfg.predict_params.checkpoint_path\n    accelerator.print(\"==\"*50)\n    accelerator.print(f\"loading model from checkpoint: {checkpoint_path}\")\n    \n    ckpt = torch.load(checkpoint_path)\n    model.load_state_dict(ckpt['state_dict'])\n    del ckpt\n    gc.collect()\n    print(\"loaded!\")\n    accelerator.print(\"### Loaded Model Weights ###\")\n    \n    model, infer_dl = accelerator.prepare(model, infer_dl)\n    \n    # run inference ---\n    sub_df = run_inference(accelerator, model, infer_dl, infer_ids)\n    \n    accelerator.wait_for_everyone()\n    \n    if accelerator.is_main_process:\n        save_path = os.path.join(save_dir, f\"{model_id}.parquet\")\n        sub_df.to_parquet(save_path)\n        accelerator.print(\"done!\")\n        accelerator.print(\"~~\"*40)\n    \nif __name__ == \"__main__\":\n    ap = argparse.ArgumentParser()\n    ap.add_argument('--config_path', type=str, required=True)\n    ap.add_argument('--save_dir', type=str, required=True)\n    ap.add_argument('--model_id', type=str, required=True)\n\n    args = ap.parse_args()\n    cfg = OmegaConf.load(args.config_path)\n\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    # execution\n    main(\n        cfg,\n        save_dir=args.save_dir,\n        model_id=args.model_id,\n    )\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T11:03:43.402458Z","iopub.execute_input":"2024-04-11T11:03:43.403275Z","iopub.status.idle":"2024-04-11T11:03:43.413196Z","shell.execute_reply.started":"2024-04-11T11:03:43.403238Z","shell.execute_reply":"2024-04-11T11:03:43.412184Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Overwriting run_deb_ranking_inference_rb.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile ./configs/conf_deb_ranking_rb.yaml\n\nmodel:\n    backbone_path: /kaggle/input/deberta-v3-large\n    max_length: 1296 # 128\n    dropout_rate: 0.0\n    gradient_checkpointing: true\n\npredict_params:\n    checkpoint_path: /kaggle/input/deberta-v3-large-ranking/detect_ai_model_last.pth.tar\n    per_device_eval_batch_size: 1","metadata":{"execution":{"iopub.status.busy":"2024-04-11T11:03:47.749278Z","iopub.execute_input":"2024-04-11T11:03:47.749975Z","iopub.status.idle":"2024-04-11T11:03:47.755602Z","shell.execute_reply.started":"2024-04-11T11:03:47.749940Z","shell.execute_reply":"2024-04-11T11:03:47.754682Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Overwriting ./configs/conf_deb_ranking_rb.yaml\n","output_type":"stream"}]},{"cell_type":"code","source":"!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 run_deb_ranking_inference_rb.py \\\n--config_path \"./configs/conf_deb_ranking_rb.yaml\" \\\n--save_dir \"./outputs\" \\\n--model_id \"m7\"\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T11:03:53.314526Z","iopub.execute_input":"2024-04-11T11:03:53.314875Z","iopub.status.idle":"2024-04-11T11:05:19.904042Z","shell.execute_reply.started":"2024-04-11T11:03:53.314847Z","shell.execute_reply":"2024-04-11T11:05:19.902894Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"2024-04-11 11:04:04.498370: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 11:04:04.498385: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 11:04:04.498426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 11:04:04.498437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 11:04:04.500073: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-04-11 11:04:04.500074: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTest csv shape: (150, 3)\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.80ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.68ba/s]\nColumn name __index_level_0__ not in the dataset. Current columns in the dataset: ['id', 'prompt_id', 'text', 'generated', 'input_ids', 'attention_mask', 'input_length']\ndata preparation done...\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.63ba/s]\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.95ba/s]\nColumn name __index_level_0__ not in the dataset. Current columns in the dataset: ['id', 'prompt_id', 'text', 'generated', 'input_ids', 'attention_mask', 'input_length']\nYou're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n################################################################################\nbatch size: 1\nshape of input_ids: torch.Size([1, 64])\nShowing 1 from a infer batch...\n\n\n\nExample 1\ninitializing the Rank Model...\nInput:\n\n[CLS] Would you ever make something that would put you in any kind of harm?. Making Drivlaby a necessity. Driverless cars must be great talk about how saving people in to today's society in way of communicating vehicles, which makes it good to actually give you our roads now.[SEP][PAD][PAD][PAD]\n================================================================================\n################################################################################\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ninitializing the Rank Model...\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n====================================================================================================\nloading model from checkpoint: /kaggle/input/deberta-v3-large-ranking/detect_ai_model_last.pth.tar\nloaded!\nloaded!\n### Loaded Model Weights ###\n100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 11.35it/s]\ndone!\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","output_type":"stream"}]},{"cell_type":"code","source":"%mkdir /kaggle/working/ghostbuster-temp\n%cd /kaggle/working/ghostbuster-temp\n%cp /kaggle/input/ghosbuster-scripts-v1/*.py .\n\n!pip uninstall -y pandas\n!pip install /kaggle/input/pip-install-many-whl/pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\n!pip uninstall -y datasets\n!pip install datasets --no-index --find-links=file:///kaggle/input/hf-ds -U -q\n\nDEBUG = 0\n\nMODEL_DIR = \"/kaggle/input/gb-overfit-rfc-svc\"\n# \"/kaggle/input/custom-gb-model-v1\"\n#\"/kaggle/input/ghostb-100k-m20\"\nvalidation_file_dir = MODEL_DIR if DEBUG else \"none\" \n\nllama_7b_path = \"/kaggle/input/llama-2/pytorch/7b-hf/1\"\ntinyllama_path = \"/kaggle/input/finetuned-tiny-llama\"\n#\"/kaggle/input/tinyllama-tinyllama-1-1b-chat-v1-0\"\n\ntext_files_dir = \"/tmp/text-files\"\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    csv_file = \"train_essays.csv\" if DEBUG else \"test_essays.csv\"\n    csv_path = \"/kaggle/input/llm-detect-ai-generated-text/\" + csv_file\nelse:\n    csv_path = \"/kaggle/input/mock-test/test.csv\"\n\n\n\nmodel1 = \"llama-7b\"\nmodel2 = \"tinyllama\"\n\nnum_rows = -1 # 500 if DEBUG else -1\n\n!python process_text.py \\\n    --csv_path $csv_path \\\n    --output_dir $text_files_dir \\\n    --num_proc 2 \\\n    --num_rows $num_rows \\\n    --validation_dir $validation_file_dir\n\n!python run_llm.py \\\n    --model_name_or_path $llama_7b_path \\\n    --path_to_text_files $text_files_dir \\\n    --batch_size 4 \\\n    --model_name $model1 \\\n    --device_map_auto\n    \n# !python run_llm.py --model_name_or_path $tinyllama_path --path_to_text_files $text_files_dir --batch_size 4 --model_name \"llama-7b\" --device_map_auto\n!accelerate launch --num_processes=2 --multi_gpu --mixed_precision=fp16 run_llm.py \\\n    --model_name_or_path $tinyllama_path \\\n    --path_to_text_files $text_files_dir \\\n    --batch_size 8 \\\n    --model_name $model2 \n\n!python run_custom_gb2.py \\\n    --model_dir $MODEL_DIR \\\n    --model1 $model1 \\\n    --model2 $model2 \\\n    --tokenizer_name \"/kaggle/input/tinyllama-tinyllama-1-1b-chat-v1-0\" \\\n    --text_dir $text_files_dir \\\n    --output_path \"gb_sub.csv\" \\\n    --debug $DEBUG \\\n    --num_proc 2\n\n!mv gb_sub.csv /kaggle/working/outputs/mgb.csv\n\n%cd /kaggle/working\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-11T11:06:08.824875Z","iopub.execute_input":"2024-04-11T11:06:08.825270Z","iopub.status.idle":"2024-04-11T11:06:39.945242Z","shell.execute_reply.started":"2024-04-11T11:06:08.825235Z","shell.execute_reply":"2024-04-11T11:06:39.944151Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"/kaggle/working/ghostbuster-temp\ncp: cannot stat '/kaggle/input/ghosbuster-scripts-v1/*.py': No such file or directory\nFound existing installation: pandas 2.1.4\nUninstalling pandas-2.1.4:\n  Successfully uninstalled pandas-2.1.4\n\u001b[33mWARNING: Requirement '/kaggle/input/pip-install-many-whl/pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n\u001b[0mProcessing /kaggle/input/pip-install-many-whl/pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/pip-install-many-whl/pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'\n\u001b[0m\u001b[31m\n\u001b[0mFound existing installation: datasets 2.1.0\nUninstalling datasets-2.1.0:\n  Successfully uninstalled datasets-2.1.0\n\u001b[33mWARNING: Location 'file:///kaggle/input/hf-ds' is ignored: it is neither a file nor a directory.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement datasets (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for datasets\u001b[0m\u001b[31m\n\u001b[0mpython: can't open file '/kaggle/working/ghostbuster-temp/process_text.py': [Errno 2] No such file or directory\npython: can't open file '/kaggle/working/ghostbuster-temp/run_llm.py': [Errno 2] No such file or directory\n/opt/conda/bin/python3.10: can't open file '/kaggle/working/ghostbuster-temp/run_llm.py': [Errno 2] No such file or directory\n/opt/conda/bin/python3.10: can't open file '/kaggle/working/ghostbuster-temp/run_llm.py': [Errno 2] No such file or directory\n[2024-04-11 11:06:37,033] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 2) local_rank: 0 (pid: 673) of binary: /opt/conda/bin/python3.10\nTraceback (most recent call last):\n  File \"/opt/conda/bin/accelerate\", line 8, in <module>\n    sys.exit(main())\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n    args.func(args)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1048, in launch_command\n    multi_gpu_launcher(args)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 702, in multi_gpu_launcher\n    distrib_run.run(args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 797, in run\n    elastic_launch(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nrun_llm.py FAILED\n------------------------------------------------------------\nFailures:\n[1]:\n  time      : 2024-04-11_11:06:37\n  host      : 6025fd9be5eb\n  rank      : 1 (local_rank: 1)\n  exitcode  : 2 (pid: 674)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-04-11_11:06:37\n  host      : 6025fd9be5eb\n  rank      : 0 (local_rank: 0)\n  exitcode  : 2 (pid: 673)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\npython: can't open file '/kaggle/working/ghostbuster-temp/run_custom_gb2.py': [Errno 2] No such file or directory\nmv: cannot stat 'gb_sub.csv': No such file or directory\n/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip uninstall pandas \n! pip install pandas\nimport pandas as pd\nsub_df_m0 = pd.read_parquet(\"./outputs/m0.parquet\")  # mistral\nsub_df_m1 = pd.read_parquet(\"./outputs/m1.parquet\")  # mistral\n\nsub_df_m2 = pd.read_parquet(\"./outputs/m2.parquet\")  # deberta-ub\n\n# sub_df_m3 = pd.read_parquet(\"./outputs/m3.parquet\")  # tf-idf\n\nsub_df_m4 = pd.read_parquet(\"./outputs/m4.parquet\")  # pl-deberta\n\n\nsub_df_m5 = pd.read_parquet(\"./outputs/m5.parquet\")  # ahmet\nsub_df_m6 = pd.read_parquet(\"./outputs/m6.parquet\")  # ahmet\n\nsub_df_m7 = pd.read_parquet(\"./outputs/m7.parquet\")  # deberta-rb\n\n#sub_df_m8 = pd.read_csv(\"./outputs/mgb.csv\")  # 👻\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T11:14:26.924878Z","iopub.execute_input":"2024-04-11T11:14:26.925257Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Found existing installation: pandas 2.2.1\nUninstalling pandas-2.2.1:\n  Would remove:\n    /opt/conda/lib/python3.10/site-packages/pandas-2.2.1.dist-info/*\n    /opt/conda/lib/python3.10/site-packages/pandas/core/_numba/extensions.py\n    /opt/conda/lib/python3.10/site-packages/pandas/core/arrays/_utils.py\n    /opt/conda/lib/python3.10/site-packages/pandas/core/arrays/arrow/accessors.py\n    /opt/conda/lib/python3.10/site-packages/pandas/io/excel/_calamine.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/apply/test_numba.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/arrays/interval/test_formats.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/arrays/interval/test_interval_pyarrow.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/arrays/interval/test_overlaps.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/copy_view/test_chained_assignment_deprecation.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/frame/methods/test_info.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/frame/test_arrow_interface.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/frame/test_repr.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/groupby/methods/*\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/groupby/test_all_methods.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/groupby/test_cumulative.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/groupby/test_numeric_only.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/groupby/test_reductions.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/categorical/test_setops.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/methods/test_asof.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/methods/test_delete.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/methods/test_map.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/methods/test_normalize.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/methods/test_resolution.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/methods/test_round.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/methods/test_to_julian_date.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/methods/test_to_pydatetime.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/methods/test_tz_convert.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/methods/test_tz_localize.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/methods/test_unique.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/test_arithmetic.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/datetimes/test_iter.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/indexes/timedeltas/test_arithmetic.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/io/formats/test_ipython_compat.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/io/test_gbq.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/io/test_http_headers.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/libs/test_libalgos.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/scalar/interval/test_constructors.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/scalar/interval/test_contains.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/scalar/interval/test_formats.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/scalar/interval/test_overlaps.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/scalar/period/test_arithmetic.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/scalar/timedelta/methods/*\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/scalar/timestamp/methods/*\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/series/accessors/test_list_accessor.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/series/accessors/test_struct_accessor.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/series/methods/test_case_when.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/series/methods/test_info.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/series/test_formats.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/tslibs/test_npy_units.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/tslibs/test_period.py\n    /opt/conda/lib/python3.10/site-packages/pandas/tests/tslibs/test_strptime.py\nProceed (Y/n)? ","output_type":"stream"}]},{"cell_type":"code","source":"Y\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # convert to rankings ---\nsub_df_m0[\"generated\"] = sub_df_m0[\"generated\"].rank(method='min')\nsub_df_m1[\"generated\"] = sub_df_m1[\"generated\"].rank(method='min')\nsub_df_m2[\"generated\"] = sub_df_m2[\"generated\"].rank(method='min')\n# sub_df_m3[\"generated\"] = sub_df_m3[\"generated\"].rank(method='min')\nsub_df_m4[\"generated\"] = sub_df_m4[\"generated\"].rank(method='min')\nsub_df_m5[\"generated\"] = sub_df_m5[\"generated\"].rank(method='min')\nsub_df_m6[\"generated\"] = sub_df_m6[\"generated\"].rank(method='min')\nsub_df_m7[\"generated\"] = sub_df_m7[\"generated\"].rank(method='min')\n#sub_df_m8[\"generated\"] = sub_df_m8[\"generated\"].rank(method='min')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.concat([\n    # mistral --- weight = 8\n    sub_df_m0,\n    sub_df_m1,\n    sub_df_m0,\n    sub_df_m1,\n    sub_df_m0,\n    sub_df_m1,\n#     sub_df_m0,\n#     sub_df_m1,\n\n    # deberta - ub\n    sub_df_m2,\n    sub_df_m2,\n    \n    # tf-idf\n#     sub_df_m3,\n#     sub_df_m3,\n    \n    # pl-mlm-deb\n    sub_df_m4,   \n    sub_df_m4,\n    \n    # ahmet --\n    sub_df_m5,\n    sub_df_m6,\n    \n    # deberta rb -\n    sub_df_m7,\n    \n    # 👻\n   # sub_df_m8,\n    #sub_df_m8,\n    #sub_df_m8,\n])\n\nsub_df = sub_df.groupby(\"id\")[\"generated\"].mean().reset_index()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}